{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b33a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(list_of_text, model):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    import torch\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    # padding = True vs padding = \"max_length\"\n",
    "    sample = tokenizer(list_of_text, padding=True, truncation=True)\n",
    "\n",
    "    sample['input_ids'] = torch.Tensor(sample['input_ids']).to(torch.int64)\n",
    "    sample['token_type_ids'] = torch.Tensor(sample['token_type_ids']).to(torch.int64)\n",
    "    sample['attention_mask'] = torch.Tensor(sample['attention_mask']).to(torch.int64)\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in sample.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d191edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(text, model, tokenizer):\n",
    "    \n",
    "    import torch\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    # padding = True vs padding = \"max_length\"\n",
    "\n",
    "    sample = tokenizer(text, padding=True, truncation=True)\n",
    "\n",
    "    sample['input_ids'] = torch.Tensor(sample['input_ids']).to(torch.int64)\n",
    "    sample['token_type_ids'] = torch.Tensor(sample['token_type_ids']).to(torch.int64)\n",
    "    sample['attention_mask'] = torch.Tensor(sample['attention_mask']).to(torch.int64)\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in sample.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**batch)\n",
    "    \n",
    "    return outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93fb174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "\n",
    "def updateDataFrame(csv_file_name, model_max_length=500):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    data = data.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    def split_sentences(list_of_words):\n",
    "        chunked_list = list()\n",
    "        chunk_size = model_max_length\n",
    "        for i in range(0, len(list_of_words), chunk_size):\n",
    "            chunked_list.append(list_of_words[i:i+chunk_size])\n",
    "\n",
    "        return chunked_list\n",
    "\n",
    "    for idx in range(len(data['data_string'])):\n",
    "#         print(data['data_string'][idx])\n",
    "#         data['data_category_number'][idx] = data['data_category_number'][idx] - 1\n",
    "        data['data_category_number'][idx] = data['data_category_number'][idx]\n",
    "        if len(str(data['data_string'][idx]).split()) > 500:\n",
    "#             print(True)\n",
    "            tempString = data['data_string'][idx]\n",
    "            tempStringSplit = tempString.split()\n",
    "            chunkedLists = split_sentences(tempStringSplit)\n",
    "        \n",
    "            for sentence in chunkedLists:\n",
    "                tempSentence = \" \".join(sentence)\n",
    "                data.loc[len(data.index)] = [\n",
    "                                            data['data_id'][idx], \n",
    "                                             tempSentence,\n",
    "                                            data['2d_coor'][idx],\n",
    "                                            data['data_title'][idx],\n",
    "                                            data['data_category'][idx],\n",
    "                                            data['data_category_number'][idx],\n",
    "                                            ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d43e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(df, number_of_labels, number_of_epochs=3):\n",
    "    from datasets import load_dataset, Dataset\n",
    "    #     dataset = load_dataset('csv', data_files='Care_Reviews.csv', split='train')\n",
    "    df = df.dropna()\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    #    dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    from datasets import DatasetDict\n",
    "    \n",
    "    train_testvalid = dataset.train_test_split()\n",
    "    test_valid = train_testvalid['test'].train_test_split()\n",
    "    \n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "    \n",
    "    dataset = train_test_valid_dataset.remove_columns(['data_id', '2d_coor', 'data_title','data_category'])\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "    # refer documentation: padding=True or padding=\"max_length\"\n",
    "        return tokenizer(examples[\"data_string\"], padding=True, truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"valid\"]\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    \n",
    "#     tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"data_category_number\", \"labels\")\n",
    "    \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    small_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "#     print(small_eval_dataset)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
    "    \n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=number_of_labels)\n",
    "    \n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from transformers import get_scheduler\n",
    "\n",
    "    num_epochs = number_of_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            print(batch)\n",
    "            outputs = model(**batch)\n",
    "            print(outputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    final_score = metric.compute()\n",
    "    return model, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "547169ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...      NaN   \n",
       "1   25451119  This experiment investigated how the esthetic ...      NaN   \n",
       "2   27236075  Women with high body dissatisfaction look less...      NaN   \n",
       "3   15010496  We have used the technique of functional MRI t...      NaN   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...      NaN   \n",
       "..       ...                                                ...      ...   \n",
       "95   9874951  Children with severe and profound disabilities...      NaN   \n",
       "96  18794733  As our ageing population demands to maintain y...      NaN   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...      NaN   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...      NaN   \n",
       "99  27265766  The aesthetic experience through art is a wind...      NaN   \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 68.36ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 360.12ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 227.85ba/s]\u001b[A\u001b[A\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([11.,  2., 17.,  3.,  3.,  4., 15.,  2.]), 'input_ids': tensor([[  101,  5689,   156,  ...,     0,     0,     0],\n",
      "        [  101,  1109,  2656,  ...,     0,     0,     0],\n",
      "        [  101,  1188,  1692,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1109, 12641,  ...,     0,     0,     0],\n",
      "        [  101, 13197,  5295,  ...,     0,     0,     0],\n",
      "        [  101,  4503, 11432,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 20]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinetuneBertSeqModelWithCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_lda.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_debug.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mfinetuneBertSeqModelWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, number_of_labels, number_of_epochs, output_file_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d_coor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#display(data)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Function that returns a fine-tuned model (fine-tuned on DSC dataset) and its score\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model, score \u001b[38;5;241m=\u001b[39m \u001b[43mfineTuneModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms the fine-tuned model: \u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the fine-tuned model on the test dataset is: \u001b[39m\u001b[38;5;124m'\u001b[39m, score)\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mfineTuneModel\u001b[0;34m(df, number_of_labels, number_of_epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m---> 78\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#             print(outputs)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m             loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1583\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1582\u001b[0m         loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1583\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1585\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/loss.py:704\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/functional.py:2980\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 2980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 20]))"
     ]
    }
   ],
   "source": [
    "finetuneBertSeqModelWithCustomDataset(input_file_name='test_lda.csv', \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=20,\n",
    "                                         number_of_epochs=5,\n",
    "                                         output_file_name=\"sample_debug.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a456915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_file(data, model, output_file_name=\"something.json\"):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    embedding_list = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        if type(data['data_string'][idx]) is float:\n",
    "            continue\n",
    "        embed = getEmbeddings([data['data_string'][idx]], model, tokenizer).tolist()[0]\n",
    "        embedding_list.append(embed)\n",
    "    \n",
    "    import numpy as np\n",
    "    embeddings_for_umap = np.array(embedding_list)\n",
    "    \n",
    "    import umap.umap_ as umap\n",
    "    umap_embedding = umap.UMAP().fit_transform(embeddings_for_umap, y=list(data['data_category_number']))\n",
    "    \n",
    "    data['2d_coor'] = umap_embedding.tolist()\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "    #     tmp_var = data_df_china_news['2d_coor'][idx].strip('][').split(', ')\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "    #     tmp_dict[\"data_x\"] = str(tmp_var[0])\n",
    "    #     tmp_dict[\"data_y\"] = str(tmp_var[1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "#         tmp_dict[\"data_string\"] = str(data['data_string'][idx])\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5a00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_masked_bert(data, num_epochs=2, number_of_labels=5):\n",
    "    from transformers import AutoTokenizer, BertForMaskedLM\n",
    "    import torch\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    data = data.dropna()\n",
    "    display(data)\n",
    "    inputs = tokenizer(list(data['data_string']), return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    \n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    \n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "        \n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "        \n",
    "    dataset = CustomDataset(inputs)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    # and move our model over to the selected device\n",
    "    model.to(device)\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    \n",
    "    from transformers import AdamW\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "    epochs = num_epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            torch.cuda.empty_cache()\n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "    model.save_pretrained('pytorch_model_unsupervised_finetuned')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9afdffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModelUnsupervised(df, number_of_labels=19, number_of_epochs=3):\n",
    "    from datasets import load_dataset, Dataset\n",
    "    #     dataset = load_dataset('csv', data_files='Care_Reviews.csv', split='train')\n",
    "    df = df.dropna()\n",
    "    display(df)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    #dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    from datasets import DatasetDict\n",
    "    \n",
    "    train_testvalid = dataset.train_test_split()\n",
    "    test_valid = train_testvalid['test'].train_test_split()\n",
    "    \n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "    \n",
    "    dataset = train_test_valid_dataset.remove_columns(['data_id', '2d_coor', 'data_title','data_category'])\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "    # refer documentation: padding=True or padding=\"max_length\"\n",
    "        return tokenizer(examples[\"data_string\"], padding=True, truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"valid\"]\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    \n",
    "#     tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"data_category_number\", \"labels\")\n",
    "    \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    small_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "#     print(small_eval_dataset)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
    "    \n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"pytorch_model_unsupervised_finetuned\", num_labels=number_of_labels)\n",
    "    \n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from transformers import get_scheduler\n",
    "\n",
    "    num_epochs = number_of_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "#             print(outputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    final_score = metric.compute()\n",
    "    return model, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are functions to delete files and directories (you will not be able to delete directories/files\n",
    "# directly from the Jupyter Notebook UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ca522c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pytorch_model_unsupervised_finetuned'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dir_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch_model_unsupervised_finetuned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/shutil.py:709\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    707\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(path)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/shutil.py:707\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     orig_st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mlstat, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pytorch_model_unsupervised_finetuned'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "dir_name = 'pytorch_model_unsupervised_finetuned'\n",
    "shutil.rmtree(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30839e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5a071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the space below to play (call) with functions initialized above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bb833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c702f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24523fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dc25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce7fb5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>journal</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2004-12-08</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>journal</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>journal</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>journal</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>[]</td>\n",
       "      <td>2004-04-07</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>journal</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2006-09-27</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>journal</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1998-12-28</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>journal</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2008-10-16</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>journal</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1985-02-15</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>journal</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>journal</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1                                                  2   3  \\\n",
       "0   15549400  journal  Effects of low-efficiency pollinators on plant...  []   \n",
       "1   25451119  journal  Electrophysiological brain dynamics during the...  []   \n",
       "2   27236075  journal  Take a look at the bright side: Effects of pos...  []   \n",
       "3   15010496  journal                       Neural correlates of beauty.  []   \n",
       "4   18648595  journal  Radiation hormesis: the good, the bad, and the...  []   \n",
       "..       ...      ...                                                ...  ..   \n",
       "95   9874951  journal  Children with severe and profound disabilities...  []   \n",
       "96  18794733  journal  \"Ethics in aesthetic nursing...avoiding the ug...  []   \n",
       "97   3984466  journal  [Subcutaneous tenotomy of the sternocleidomast...  []   \n",
       "98  30231330  journal  Acute Myeloid Leukemia: The Good, the Bad, and...  []   \n",
       "99  27265766  journal  Ugly aesthetic perception associated with emot...  []   \n",
       "\n",
       "             4                                                  5 6   \n",
       "0   2004-12-08  Floral visitors vary in their pollination effi...     \n",
       "1   2015-01-12  This experiment investigated how the esthetic ...     \n",
       "2   2016-08-04  Women with high body dissatisfaction look less...     \n",
       "3   2004-04-07  We have used the technique of functional MRI t...     \n",
       "4   2006-09-27  Three aspects of hormesis with low doses of io...     \n",
       "..         ...                                                ... ..  \n",
       "95  1998-12-28  Children with severe and profound disabilities...     \n",
       "96  2008-10-16  As our ageing population demands to maintain y...     \n",
       "97  1985-02-15  This study reviews the cases of 49 patients wi...     \n",
       "98        None  Acute myeloid leukemia (AML) was initially sub...     \n",
       "99  2016-08-05  The aesthetic experience through art is a wind...     \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td></td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td></td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td></td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td></td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td></td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td></td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td></td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td></td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td></td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td></td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string 2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...           \n",
       "1   25451119  This experiment investigated how the esthetic ...           \n",
       "2   27236075  Women with high body dissatisfaction look less...           \n",
       "3   15010496  We have used the technique of functional MRI t...           \n",
       "4   18648595  Three aspects of hormesis with low doses of io...           \n",
       "..       ...                                                ...     ...   \n",
       "95   9874951  Children with severe and profound disabilities...           \n",
       "96  18794733  As our ageing population demands to maintain y...           \n",
       "97   3984466  This study reviews the cases of 49 patients wi...           \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...           \n",
       "99  27265766  The aesthetic experience through art is a wind...           \n",
       "\n",
       "                                           data_title data_category  \\\n",
       "0   Effects of low-efficiency pollinators on plant...                 \n",
       "1   Electrophysiological brain dynamics during the...                 \n",
       "2   Take a look at the bright side: Effects of pos...                 \n",
       "3                        Neural correlates of beauty.                 \n",
       "4   Radiation hormesis: the good, the bad, and the...                 \n",
       "..                                                ...           ...   \n",
       "95  Children with severe and profound disabilities...                 \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...                 \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...                 \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...                 \n",
       "99  Ugly aesthetic perception associated with emot...                 \n",
       "\n",
       "   data_category_number  \n",
       "0                        \n",
       "1                        \n",
       "2                        \n",
       "3                        \n",
       "4                        \n",
       "..                  ...  \n",
       "95                       \n",
       "96                       \n",
       "97                       \n",
       "98                       \n",
       "99                       \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "data = pd.read_json('pubmed_1k.json')\n",
    "display(data)\n",
    "\n",
    "\n",
    "data = data.rename(columns={2:'data_title', 5:'data_string',0:'data_id'})\n",
    "data\n",
    "data = data.drop(columns=[1,3,4,6])\n",
    "data\n",
    "data['2d_coor'] = ''\n",
    "data['data_category'] = ''\n",
    "data['data_category_number'] = ''\n",
    "data = data.reindex(columns=['data_id', 'data_string', '2d_coor', 'data_title', 'data_category', 'data_category_number'])\n",
    "data.dropna()\n",
    "display(data)\n",
    "data.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f233888",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1 ###\n",
    "\n",
    "# This function will create a csv in a format (mostly changing columns names) that we need for training models\n",
    "# The following columns are needed: data_category_number, data_title, data_string, data_category, data_id, 2d_coor\n",
    "\n",
    "# Note: This function will have to be modified according to the need as not all datasets have labels but above\n",
    "# mentioned columns should be there\n",
    "def create_structured_csv(csv_file_name):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    #data = data.drop(columns=['Unnamed: 0', 'year'])\n",
    "    data = data.rename(columns={'seq':'data_category_number', 'title':'data_title', 'abstract':'data_string', 'CODE':'data_category','id':'data_id'})\n",
    "    data['2d_coor'] = ''\n",
    "    data = data.reindex(columns=['data_id', 'data_string', '2d_coor', 'data_title', 'data_category', 'data_category_number'])\n",
    "    data.to_csv('test_cleaned.csv')\n",
    "    return data\n",
    "\n",
    "csv_file_name = 'test.csv'\n",
    "data = create_structured_csv(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17eabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2 ###\n",
    "\n",
    "# Not all the datasets at DSC are labelled. Hence, we need to label some and we use LDA for that\n",
    "def apply_lda_on_dataset(df):\n",
    "    import gensim\n",
    "    from gensim.utils import simple_preprocess\n",
    "    from gensim.parsing.preprocessing import STOPWORDS\n",
    "#     from nltk.stem.porter import *\n",
    "    from gensim import corpora, models\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    np.random.seed(2018)\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Function that preprocesses all text documents before feeding to lda model\n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        i = 0\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    # the file name should be the one generated from the function above     \n",
    "#     data = pd.read_csv('Care_Reviews.csv')\n",
    "    data = df\n",
    "    documents = data\n",
    "    documents = documents.dropna(subset=['data_string'])\n",
    "    processed_docs = documents['data_string'].map(preprocess)\n",
    "    \n",
    "    # Creates a dictionary from the documents (Note: Here the argument 'preprocessed_docs' is a 'list of lists')\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # Creates a bag_of_words corpus     \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # Creates a tfidf matrix/table required for training\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    # Trains an lda model with tfidf\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "    \n",
    "    for idx in range(len(data['data_string'])):\n",
    "        data_string = data['data_string'][idx]\n",
    "#         print(type(data_string))\n",
    "#         print(dictionary.doc2bow(preprocess(data_string)))\n",
    "#         print(lda_model_tfidf.get_document_topics())\n",
    "        data_string_topic_no = lda_model_tfidf.get_document_topics(dictionary.doc2bow(preprocess(data_string)))[0][0]\n",
    "#         print(data_string_topic_no)\n",
    "        data_string_topic = lda_model_tfidf.print_topic(data_string_topic_no)\n",
    "        data['data_category'][idx] = data_string_topic\n",
    "        data_string_topic_num = lda_model_tfidf.get_document_topics(dictionary.doc2bow(preprocess(data_string)))[0][0]\n",
    "        data['data_category_number'][idx] = data_string_topic_num\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7353465f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ez/nltk_data...\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1345159822.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category'][idx] = data_string_topic\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1345159822.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data_string_topic_num\n"
     ]
    }
   ],
   "source": [
    "data = apply_lda_on_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a500a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('test_lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0db8cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is responsible for fine-tuning an existing Bert Model (from huggingface) with a DSC dataset\n",
    "def finetuneBertSeqModelWithCustomDataset(input_file_name='test_lda.csv', \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=15,\n",
    "                                         number_of_epochs=15,\n",
    "                                         output_file_name=\"something.json\"):\n",
    "    # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)     \n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    #data = data.drop(['Unnamed: 0']\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    #display(data)\n",
    "    \n",
    "    # Function that returns a fine-tuned model (fine-tuned on DSC dataset) and its score\n",
    "    model, score = fineTuneModel(df=data, number_of_labels=number_of_labels, number_of_epochs=number_of_epochs)\n",
    "    \n",
    "    print('Here\\s the fine-tuned model: ', model)\n",
    "    print('Accuracy of the fine-tuned model on the test dataset is: ', score)\n",
    "    \n",
    "    # Function that returns the dataframe with embeddings (UMAP reduces high dimensional embedding to 2D)     \n",
    "    data_with_embeddings = get_json_file(data, model, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9ea842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...      NaN   \n",
       "1   25451119  This experiment investigated how the esthetic ...      NaN   \n",
       "2   27236075  Women with high body dissatisfaction look less...      NaN   \n",
       "3   15010496  We have used the technique of functional MRI t...      NaN   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...      NaN   \n",
       "..       ...                                                ...      ...   \n",
       "95   9874951  Children with severe and profound disabilities...      NaN   \n",
       "96  18794733  As our ageing population demands to maintain y...      NaN   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...      NaN   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...      NaN   \n",
       "99  27265766  The aesthetic experience through art is a wind...      NaN   \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 71.73ba/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 292.55ba/s]\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 187.69ba/s]\u001b[A\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|                                                    | 0/50 [03:33<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 20]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinetuneBertSeqModelWithCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_lda.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_debug.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mfinetuneBertSeqModelWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, number_of_labels, number_of_epochs, output_file_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d_coor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#display(data)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Function that returns a fine-tuned model (fine-tuned on DSC dataset) and its score\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model, score \u001b[38;5;241m=\u001b[39m \u001b[43mfineTuneModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms the fine-tuned model: \u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the fine-tuned model on the test dataset is: \u001b[39m\u001b[38;5;124m'\u001b[39m, score)\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mfineTuneModel\u001b[0;34m(df, number_of_labels, number_of_epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     76\u001b[0m             batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 77\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#             print(outputs)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m             loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1583\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1582\u001b[0m         loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1583\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1585\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/loss.py:704\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/functional.py:2980\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 2980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 20]))"
     ]
    }
   ],
   "source": [
    "finetuneBertSeqModelWithCustomDataset(input_file_name='test_lda.csv', \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=20,\n",
    "                                         number_of_epochs=5,\n",
    "                                         output_file_name=\"sample_debug.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f66c6e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3938428121.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_2937/3938428121.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    output_file_name=\"something.json\"):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# The following function is responsible for pretraining a masked bert model and fine-tuning the same pretrained model \n",
    "# (from huggingface) with a DSC dataset\n",
    "def finetuneBertModelAfterPretrainingOfMaskedBertWithCustomDataset(input_file_name='ultra_clean_abstract_with_text.csv', \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=19,\n",
    "                                         number_of_epochs_for_masked_bert=2,\n",
    "                                         number_of_epochs_for_finetuning_masked_bert=5                   \n",
    "                                         output_file_name=\"something.json\"):\n",
    "    \n",
    "    # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Function that pretrains a masked bert model and saves that model in a directory: 'pytorch_model_unsupervised_finetuned'\n",
    "    train_masked_bert(data, num_epochs=number_of_epochs_for_masked_bert, number_of_labels=number_of_labels)\n",
    "    \n",
    "    # Function that fine-tunes the above pretrained masked bert model\n",
    "    new_model, score = fineTuneModelUnsupervised(df, number_of_labels=number_of_labels, number_of_epochs=number_of_epochs_for_finetuning_masked_bert)\n",
    "    \n",
    "    print('Here\\s the fine-tuned model: ', model)\n",
    "    print('Accuracy of the fine-tuned model on the test dataset is: ', score)\n",
    "    \n",
    "    # Function that returns the dataframe with embeddings (UMAP reduces high dimensional embedding to 2D)     \n",
    "    data_with_embeddings = get_json_file(data, model, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ed93b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UMAPWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         use_labels=True,\n",
    "                         sentence_transformer_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_umap = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with UMAP\n",
    "    import umap.umap_ as umap\n",
    "    umap_embedding = umap.UMAP().fit_transform(embeddings_for_umap, y=list(data['data_category_number']) if use_labels else None)\n",
    "    \n",
    "    data['2d_coor'] = umap_embedding.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f293c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...      NaN   \n",
       "1   25451119  This experiment investigated how the esthetic ...      NaN   \n",
       "2   27236075  Women with high body dissatisfaction look less...      NaN   \n",
       "3   15010496  We have used the technique of functional MRI t...      NaN   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...      NaN   \n",
       "..       ...                                                ...      ...   \n",
       "95   9874951  Children with severe and profound disabilities...      NaN   \n",
       "96  18794733  As our ageing population demands to maintain y...      NaN   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...      NaN   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...      NaN   \n",
       "99  27265766  The aesthetic experience through art is a wind...      NaN   \n",
       "\n",
       "                                           data_title  data_category  \\\n",
       "0   Effects of low-efficiency pollinators on plant...            NaN   \n",
       "1   Electrophysiological brain dynamics during the...            NaN   \n",
       "2   Take a look at the bright side: Effects of pos...            NaN   \n",
       "3                        Neural correlates of beauty.            NaN   \n",
       "4   Radiation hormesis: the good, the bad, and the...            NaN   \n",
       "..                                                ...            ...   \n",
       "95  Children with severe and profound disabilities...            NaN   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...            NaN   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...            NaN   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...            NaN   \n",
       "99  Ugly aesthetic perception associated with emot...            NaN   \n",
       "\n",
       "    data_category_number  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "..                   ...  \n",
       "95                   NaN  \n",
       "96                   NaN  \n",
       "97                   NaN  \n",
       "98                   NaN  \n",
       "99                   NaN  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td></td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td></td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td></td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td></td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td></td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td></td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td></td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td></td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td></td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td></td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string 2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...           \n",
       "1   25451119  This experiment investigated how the esthetic ...           \n",
       "2   27236075  Women with high body dissatisfaction look less...           \n",
       "3   15010496  We have used the technique of functional MRI t...           \n",
       "4   18648595  Three aspects of hormesis with low doses of io...           \n",
       "..       ...                                                ...     ...   \n",
       "95   9874951  Children with severe and profound disabilities...           \n",
       "96  18794733  As our ageing population demands to maintain y...           \n",
       "97   3984466  This study reviews the cases of 49 patients wi...           \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...           \n",
       "99  27265766  The aesthetic experience through art is a wind...           \n",
       "\n",
       "                                           data_title  data_category  \\\n",
       "0   Effects of low-efficiency pollinators on plant...            NaN   \n",
       "1   Electrophysiological brain dynamics during the...            NaN   \n",
       "2   Take a look at the bright side: Effects of pos...            NaN   \n",
       "3                        Neural correlates of beauty.            NaN   \n",
       "4   Radiation hormesis: the good, the bad, and the...            NaN   \n",
       "..                                                ...            ...   \n",
       "95  Children with severe and profound disabilities...            NaN   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...            NaN   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...            NaN   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...            NaN   \n",
       "99  Ugly aesthetic perception associated with emot...            NaN   \n",
       "\n",
       "    data_category_number  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "..                   ...  \n",
       "95                   NaN  \n",
       "96                   NaN  \n",
       "97                   NaN  \n",
       "98                   NaN  \n",
       "99                   NaN  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [40:36<?, ?it/s]\n",
      "  0%|                                                    | 0/50 [14:31<?, ?it/s]\n",
      "  0%|                                                    | 0/50 [07:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(numpy 1.23.0rc3 (/Users/ez/miniconda3/envs/torch-nightly/lib/python3.8/site-packages), Requirement.parse('numpy<1.23.0,>=1.16.5'), {'scipy'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mUMAPWithCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_json_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_umap.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43muse_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m d\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mUMAPWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, output_json_file_name, use_labels, sentence_transformer_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m embeddings_for_umap \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Reducing the dimensionality of embeddings with UMAP\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m     24\u001b[0m umap_embedding \u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP()\u001b[38;5;241m.\u001b[39mfit_transform(embeddings_for_umap, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_category_number\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m use_labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d_coor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m umap_embedding\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/umap/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/umap/umap_.py:47\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spectral_layout\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     optimize_layout_euclidean,\n\u001b[1;32m     43\u001b[0m     optimize_layout_generic,\n\u001b[1;32m     44\u001b[0m     optimize_layout_inverse,\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynndescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNDescent\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynndescent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m named_distances \u001b[38;5;28;01mas\u001b[39;00m pynn_named_distances\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynndescent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_named_distances \u001b[38;5;28;01mas\u001b[39;00m pynn_sparse_named_distances\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/pynndescent/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# might be a missing symbol due to e.g. tbb libraries missing\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         numba\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTHREADING_LAYER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpynndescent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/pkg_resources/__init__.py:477\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    475\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 477\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/pkg_resources/__init__.py:353\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/pkg_resources/__init__.py:897\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements):\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/pkg_resources/__init__.py:788\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m req:\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     dependent_req \u001b[38;5;241m=\u001b[39m required_by[req]\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m    791\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrequires(req\u001b[38;5;241m.\u001b[39mextras)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (numpy 1.23.0rc3 (/Users/ez/miniconda3/envs/torch-nightly/lib/python3.8/site-packages), Requirement.parse('numpy<1.23.0,>=1.16.5'), {'scipy'})"
     ]
    }
   ],
   "source": [
    "d = UMAPWithCustomDataset(input_file_name='test_cleaned.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test_umap.json',\n",
    "                         use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11f50df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                         use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_tsne = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with TSNE\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(embeddings_for_tsne, list(data['data_category_number']) if use_labels else None)\n",
    "    \n",
    "    data['2d_coor'] = tsne_results.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58445cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3355/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...      NaN   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...      NaN   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...      NaN   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...      NaN   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...      NaN   \n",
       "...         ...                                                ...      ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...      NaN   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...      NaN   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...      NaN   \n",
       "193490   813146  work of experts, including independent auditor...      NaN   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...      NaN   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td></td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td></td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td></td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td></td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td></td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string 2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...           \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...           \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...           \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...           \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...           \n",
       "...         ...                                                ...     ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...           \n",
       "193488   813146  Individually or Collectively, Lead to Negative...           \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...           \n",
       "193490   813146  work of experts, including independent auditor...           \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...           \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 193492 samples in 0.069s...\n",
      "[t-SNE] Computed neighbors for 193492 samples in 1197.803s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 101000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 102000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 103000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 104000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 105000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 106000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 107000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 108000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 109000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 110000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 111000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 112000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 113000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 114000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 115000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 116000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 117000 / 193492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 118000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 119000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 120000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 121000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 122000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 123000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 124000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 125000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 126000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 127000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 128000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 129000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 130000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 131000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 132000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 133000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 134000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 135000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 136000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 137000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 138000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 139000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 140000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 141000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 142000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 143000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 144000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 145000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 146000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 147000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 148000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 149000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 150000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 151000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 152000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 153000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 154000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 155000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 156000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 157000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 158000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 159000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 160000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 161000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 162000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 163000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 164000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 165000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 166000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 167000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 168000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 169000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 170000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 171000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 172000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 173000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 174000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 175000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 176000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 177000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 178000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 179000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 180000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 181000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 182000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 183000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 184000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 185000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 186000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 187000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 188000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 189000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 190000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 191000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 192000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 193000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 193492 / 193492\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 50 iterations with early exaggeration: 128.080566\n",
      "[t-SNE] KL divergence after 300 iterations: 5.640841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[4.397582054138184, -3.5684354305267334]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.3279964327812195, 8.61110782623291]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[4.169193744659424, -4.005739212036133]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[-2.864100933074951, -0.9488449692726135]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[0.2897432744503021, 10.175537109375]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[-5.589747428894043, 3.172562599182129]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[4.083289623260498, 9.9985933303833]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[4.8006367683410645, 9.934613227844238]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[4.684049606323242, 9.922048568725586]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[12.157347679138184, 0.8815428614616394]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                          2d_coor  \\\n",
       "0        [4.397582054138184, -3.5684354305267334]   \n",
       "1         [-0.3279964327812195, 8.61110782623291]   \n",
       "2         [4.169193744659424, -4.005739212036133]   \n",
       "3       [-2.864100933074951, -0.9488449692726135]   \n",
       "4           [0.2897432744503021, 10.175537109375]   \n",
       "...                                           ...   \n",
       "193487    [-5.589747428894043, 3.172562599182129]   \n",
       "193488       [4.083289623260498, 9.9985933303833]   \n",
       "193489    [4.8006367683410645, 9.934613227844238]   \n",
       "193490     [4.684049606323242, 9.922048568725586]   \n",
       "193491   [12.157347679138184, 0.8815428614616394]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[4.397582054138184, -3.5684354305267334]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.3279964327812195, 8.61110782623291]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[4.169193744659424, -4.005739212036133]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[-2.864100933074951, -0.9488449692726135]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[0.2897432744503021, 10.175537109375]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[-5.589747428894043, 3.172562599182129]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[4.083289623260498, 9.9985933303833]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[4.8006367683410645, 9.934613227844238]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[4.684049606323242, 9.922048568725586]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[12.157347679138184, 0.8815428614616394]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                          2d_coor  \\\n",
       "0        [4.397582054138184, -3.5684354305267334]   \n",
       "1         [-0.3279964327812195, 8.61110782623291]   \n",
       "2         [4.169193744659424, -4.005739212036133]   \n",
       "3       [-2.864100933074951, -0.9488449692726135]   \n",
       "4           [0.2897432744503021, 10.175537109375]   \n",
       "...                                           ...   \n",
       "193487    [-5.589747428894043, 3.172562599182129]   \n",
       "193488       [4.083289623260498, 9.9985933303833]   \n",
       "193489    [4.8006367683410645, 9.934613227844238]   \n",
       "193490     [4.684049606323242, 9.922048568725586]   \n",
       "193491   [12.157347679138184, 0.8815428614616394]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = TSNEWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_tsne.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                         use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "317202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_pca = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_pca, list(data['data_category_number']) if use_labels else None)\n",
    "\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_pca)\n",
    "    \n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13f93324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_82464/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...      NaN   \n",
       "1   25451119  This experiment investigated how the esthetic ...      NaN   \n",
       "2   27236075  Women with high body dissatisfaction look less...      NaN   \n",
       "3   15010496  We have used the technique of functional MRI t...      NaN   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...      NaN   \n",
       "..       ...                                                ...      ...   \n",
       "95   9874951  Children with severe and profound disabilities...      NaN   \n",
       "96  18794733  As our ageing population demands to maintain y...      NaN   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...      NaN   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...      NaN   \n",
       "99  27265766  The aesthetic experience through art is a wind...      NaN   \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td></td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td></td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td></td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td></td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td></td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td></td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td></td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td></td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td></td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td></td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string 2d_coor  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...           \n",
       "1   25451119  This experiment investigated how the esthetic ...           \n",
       "2   27236075  Women with high body dissatisfaction look less...           \n",
       "3   15010496  We have used the technique of functional MRI t...           \n",
       "4   18648595  Three aspects of hormesis with low doses of io...           \n",
       "..       ...                                                ...     ...   \n",
       "95   9874951  Children with severe and profound disabilities...           \n",
       "96  18794733  As our ageing population demands to maintain y...           \n",
       "97   3984466  This study reviews the cases of 49 patients wi...           \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...           \n",
       "99  27265766  The aesthetic experience through art is a wind...           \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████| 1.18k/1.18k [00:00<00:00, 228kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 190/190 [00:00<00:00, 56.2kB/s]\n",
      "Downloading: 100%|█████████████████████████| 10.2k/10.2k [00:00<00:00, 2.15MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 612/612 [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 116/116 [00:00<00:00, 19.1kB/s]\n",
      "Downloading: 100%|██████████████████████████| 39.3k/39.3k [00:00<00:00, 642kB/s]\n",
      "Downloading: 100%|█████████████████████████| 90.9M/90.9M [00:09<00:00, 9.44MB/s]\n",
      "Downloading: 100%|███████████████████████████| 53.0/53.0 [00:00<00:00, 6.57kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 112/112 [00:00<00:00, 21.1kB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 1.59MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 350/350 [00:00<00:00, 36.8kB/s]\n",
      "Downloading: 100%|█████████████████████████| 13.2k/13.2k [00:00<00:00, 2.14MB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 1.34MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 349/349 [00:00<00:00, 58.8kB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>[0.27713698148727417, -0.10882366448640823]</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>[0.5933237075805664, 0.09017793834209442]</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>[0.5550868511199951, 0.047366585582494736]</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>[0.4833657741546631, 0.09579300880432129]</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>[-0.14024655520915985, -0.2112492322921753]</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>[-0.21721360087394714, -0.22595451772212982]</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>[0.06945352256298065, -0.00463707372546196]</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>[-0.3445499837398529, 0.38034558296203613]</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>[-0.20531556010246277, -0.16500544548034668]</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>[0.4516730308532715, 0.01466207392513752]</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...   \n",
       "1   25451119  This experiment investigated how the esthetic ...   \n",
       "2   27236075  Women with high body dissatisfaction look less...   \n",
       "3   15010496  We have used the technique of functional MRI t...   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...   \n",
       "..       ...                                                ...   \n",
       "95   9874951  Children with severe and profound disabilities...   \n",
       "96  18794733  As our ageing population demands to maintain y...   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...   \n",
       "99  27265766  The aesthetic experience through art is a wind...   \n",
       "\n",
       "                                         2d_coor  \\\n",
       "0    [0.27713698148727417, -0.10882366448640823]   \n",
       "1      [0.5933237075805664, 0.09017793834209442]   \n",
       "2     [0.5550868511199951, 0.047366585582494736]   \n",
       "3      [0.4833657741546631, 0.09579300880432129]   \n",
       "4    [-0.14024655520915985, -0.2112492322921753]   \n",
       "..                                           ...   \n",
       "95  [-0.21721360087394714, -0.22595451772212982]   \n",
       "96   [0.06945352256298065, -0.00463707372546196]   \n",
       "97    [-0.3445499837398529, 0.38034558296203613]   \n",
       "98  [-0.20531556010246277, -0.16500544548034668]   \n",
       "99     [0.4516730308532715, 0.01466207392513752]   \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15549400</td>\n",
       "      <td>Floral visitors vary in their pollination effi...</td>\n",
       "      <td>[0.27713698148727417, -0.10882366448640823]</td>\n",
       "      <td>Effects of low-efficiency pollinators on plant...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25451119</td>\n",
       "      <td>This experiment investigated how the esthetic ...</td>\n",
       "      <td>[0.5933237075805664, 0.09017793834209442]</td>\n",
       "      <td>Electrophysiological brain dynamics during the...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27236075</td>\n",
       "      <td>Women with high body dissatisfaction look less...</td>\n",
       "      <td>[0.5550868511199951, 0.047366585582494736]</td>\n",
       "      <td>Take a look at the bright side: Effects of pos...</td>\n",
       "      <td>0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15010496</td>\n",
       "      <td>We have used the technique of functional MRI t...</td>\n",
       "      <td>[0.4833657741546631, 0.09579300880432129]</td>\n",
       "      <td>Neural correlates of beauty.</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18648595</td>\n",
       "      <td>Three aspects of hormesis with low doses of io...</td>\n",
       "      <td>[-0.14024655520915985, -0.2112492322921753]</td>\n",
       "      <td>Radiation hormesis: the good, the bad, and the...</td>\n",
       "      <td>0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9874951</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>[-0.21721360087394714, -0.22595451772212982]</td>\n",
       "      <td>Children with severe and profound disabilities...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>18794733</td>\n",
       "      <td>As our ageing population demands to maintain y...</td>\n",
       "      <td>[0.06945352256298065, -0.00463707372546196]</td>\n",
       "      <td>\"Ethics in aesthetic nursing...avoiding the ug...</td>\n",
       "      <td>0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3984466</td>\n",
       "      <td>This study reviews the cases of 49 patients wi...</td>\n",
       "      <td>[-0.3445499837398529, 0.38034558296203613]</td>\n",
       "      <td>[Subcutaneous tenotomy of the sternocleidomast...</td>\n",
       "      <td>0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>30231330</td>\n",
       "      <td>Acute myeloid leukemia (AML) was initially sub...</td>\n",
       "      <td>[-0.20531556010246277, -0.16500544548034668]</td>\n",
       "      <td>Acute Myeloid Leukemia: The Good, the Bad, and...</td>\n",
       "      <td>0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27265766</td>\n",
       "      <td>The aesthetic experience through art is a wind...</td>\n",
       "      <td>[0.4516730308532715, 0.01466207392513752]</td>\n",
       "      <td>Ugly aesthetic perception associated with emot...</td>\n",
       "      <td>0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_id                                        data_string  \\\n",
       "0   15549400  Floral visitors vary in their pollination effi...   \n",
       "1   25451119  This experiment investigated how the esthetic ...   \n",
       "2   27236075  Women with high body dissatisfaction look less...   \n",
       "3   15010496  We have used the technique of functional MRI t...   \n",
       "4   18648595  Three aspects of hormesis with low doses of io...   \n",
       "..       ...                                                ...   \n",
       "95   9874951  Children with severe and profound disabilities...   \n",
       "96  18794733  As our ageing population demands to maintain y...   \n",
       "97   3984466  This study reviews the cases of 49 patients wi...   \n",
       "98  30231330  Acute myeloid leukemia (AML) was initially sub...   \n",
       "99  27265766  The aesthetic experience through art is a wind...   \n",
       "\n",
       "                                         2d_coor  \\\n",
       "0    [0.27713698148727417, -0.10882366448640823]   \n",
       "1      [0.5933237075805664, 0.09017793834209442]   \n",
       "2     [0.5550868511199951, 0.047366585582494736]   \n",
       "3      [0.4833657741546631, 0.09579300880432129]   \n",
       "4    [-0.14024655520915985, -0.2112492322921753]   \n",
       "..                                           ...   \n",
       "95  [-0.21721360087394714, -0.22595451772212982]   \n",
       "96   [0.06945352256298065, -0.00463707372546196]   \n",
       "97    [-0.3445499837398529, 0.38034558296203613]   \n",
       "98  [-0.20531556010246277, -0.16500544548034668]   \n",
       "99     [0.4516730308532715, 0.01466207392513752]   \n",
       "\n",
       "                                           data_title  \\\n",
       "0   Effects of low-efficiency pollinators on plant...   \n",
       "1   Electrophysiological brain dynamics during the...   \n",
       "2   Take a look at the bright side: Effects of pos...   \n",
       "3                        Neural correlates of beauty.   \n",
       "4   Radiation hormesis: the good, the bad, and the...   \n",
       "..                                                ...   \n",
       "95  Children with severe and profound disabilities...   \n",
       "96  \"Ethics in aesthetic nursing...avoiding the ug...   \n",
       "97  [Subcutaneous tenotomy of the sternocleidomast...   \n",
       "98  Acute Myeloid Leukemia: The Good, the Bad, and...   \n",
       "99  Ugly aesthetic perception associated with emot...   \n",
       "\n",
       "                                        data_category  data_category_number  \n",
       "0   0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "1   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "2   0.004*\"parallel\" + 0.003*\"work\" + 0.003*\"herba...                   9.0  \n",
       "3   0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "4   0.005*\"failure\" + 0.004*\"pollinators\" + 0.003*...                   5.0  \n",
       "..                                                ...                   ...  \n",
       "95  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "96  0.004*\"west\" + 0.003*\"nonsurgical\" + 0.002*\"ae...                   1.0  \n",
       "97  0.003*\"challenging\" + 0.003*\"bonds\" + 0.003*\"c...                   7.0  \n",
       "98  0.004*\"mood\" + 0.003*\"backfilling\" + 0.002*\"in...                   4.0  \n",
       "99  0.003*\"disabilities\" + 0.003*\"aesthetic\" + 0.0...                   3.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = PCAWithCustomDataset(input_file_name='test_lda.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test_pca.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1946061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansAndPCAWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_kmeans = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA After applying KMeans\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=len(data['data_category_number'].unique()))\n",
    "    labels = kmeans.fit_predict(embeddings_for_kmeans, list(data['data_category_number']) if use_labels else None)\n",
    "    labels_scale = kmeans.labels_\n",
    "\n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_kmeans, labels.tolist())\n",
    "\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_kmeans)\n",
    "    \n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    new_labels = labels.tolist()\n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "#         tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_category_number\"] = str(new_labels[idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a03b04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1741/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...      NaN   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...      NaN   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...      NaN   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...      NaN   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...      NaN   \n",
       "...         ...                                                ...      ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...      NaN   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...      NaN   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...      NaN   \n",
       "193490   813146  work of experts, including independent auditor...      NaN   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...      NaN   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td></td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td></td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td></td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td></td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td></td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string 2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...           \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...           \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...           \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...           \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...           \n",
       "...         ...                                                ...     ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...           \n",
       "193488   813146  Individually or Collectively, Lead to Negative...           \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...           \n",
       "193490   813146  work of experts, including independent auditor...           \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...           \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[-0.21102222800254822, -0.1435994803905487]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[0.1290389597415924, 0.03235282376408577]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[-0.14857149124145508, -0.17933329939842224]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[0.3369511067867279, -0.1775447428226471]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[-0.026973845437169075, 0.012201634235680103]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[0.3518417477607727, -0.1885872483253479]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[0.3118555545806885, -0.04097635671496391]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[0.1637514978647232, 0.08637033402919769]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[0.14689454436302185, 0.08255422115325928]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[0.29277899861335754, -0.10368742793798447]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                              2d_coor  \\\n",
       "0         [-0.21102222800254822, -0.1435994803905487]   \n",
       "1           [0.1290389597415924, 0.03235282376408577]   \n",
       "2        [-0.14857149124145508, -0.17933329939842224]   \n",
       "3           [0.3369511067867279, -0.1775447428226471]   \n",
       "4       [-0.026973845437169075, 0.012201634235680103]   \n",
       "...                                               ...   \n",
       "193487      [0.3518417477607727, -0.1885872483253479]   \n",
       "193488     [0.3118555545806885, -0.04097635671496391]   \n",
       "193489      [0.1637514978647232, 0.08637033402919769]   \n",
       "193490     [0.14689454436302185, 0.08255422115325928]   \n",
       "193491    [0.29277899861335754, -0.10368742793798447]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[-0.21102222800254822, -0.1435994803905487]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[0.1290389597415924, 0.03235282376408577]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[-0.14857149124145508, -0.17933329939842224]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[0.3369511067867279, -0.1775447428226471]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[-0.026973845437169075, 0.012201634235680103]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[0.3518417477607727, -0.1885872483253479]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[0.3118555545806885, -0.04097635671496391]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[0.1637514978647232, 0.08637033402919769]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[0.14689454436302185, 0.08255422115325928]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[0.29277899861335754, -0.10368742793798447]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                              2d_coor  \\\n",
       "0         [-0.21102222800254822, -0.1435994803905487]   \n",
       "1           [0.1290389597415924, 0.03235282376408577]   \n",
       "2        [-0.14857149124145508, -0.17933329939842224]   \n",
       "3           [0.3369511067867279, -0.1775447428226471]   \n",
       "4       [-0.026973845437169075, 0.012201634235680103]   \n",
       "...                                               ...   \n",
       "193487      [0.3518417477607727, -0.1885872483253479]   \n",
       "193488     [0.3118555545806885, -0.04097635671496391]   \n",
       "193489      [0.1637514978647232, 0.08637033402919769]   \n",
       "193490     [0.14689454436302185, 0.08255422115325928]   \n",
       "193491    [0.29277899861335754, -0.10368742793798447]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = KMeansAndPCAWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_kmeanspca.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAAndKMeansWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_kmeans = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA After applying KMeans\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_kmeans, list(data['data_category_number']) if use_labels else None)\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_kmeans)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=len(data['data_category_number'].unique()))\n",
    "    labels = kmeans.fit_predict(pca_2d, list(data['data_category_number']) if use_labels else None)\n",
    "    labels_scale = kmeans.labels_\n",
    "\n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    new_labels = labels.tolist()\n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "#         tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_category_number\"] = str(new_labels[idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = PCAAndKMeansWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_pcakmeans.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "torch-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
