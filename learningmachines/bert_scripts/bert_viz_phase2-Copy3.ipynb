{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b33a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(list_of_text, model):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    import torch\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    # padding = True vs padding = \"max_length\"\n",
    "    sample = tokenizer(list_of_text, padding=True, truncation=True)\n",
    "\n",
    "    sample['input_ids'] = torch.Tensor(sample['input_ids']).to(torch.int64)\n",
    "    sample['token_type_ids'] = torch.Tensor(sample['token_type_ids']).to(torch.int64)\n",
    "    sample['attention_mask'] = torch.Tensor(sample['attention_mask']).to(torch.int64)\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in sample.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d191edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(text, model, tokenizer):\n",
    "    \n",
    "    import torch\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    # padding = True vs padding = \"max_length\"\n",
    "\n",
    "    sample = tokenizer(text, padding=True, truncation=True)\n",
    "\n",
    "    sample['input_ids'] = torch.Tensor(sample['input_ids']).to(torch.int64)\n",
    "    sample['token_type_ids'] = torch.Tensor(sample['token_type_ids']).to(torch.int64)\n",
    "    sample['attention_mask'] = torch.Tensor(sample['attention_mask']).to(torch.int64)\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in sample.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**batch)\n",
    "    \n",
    "    return outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93fb174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "\n",
    "def updateDataFrame(csv_file_name, model_max_length=500):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    #data = data.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    def split_sentences(list_of_words):\n",
    "        chunked_list = list()\n",
    "        chunk_size = model_max_length\n",
    "        for i in range(0, len(list_of_words), chunk_size):\n",
    "            chunked_list.append(list_of_words[i:i+chunk_size])\n",
    "\n",
    "        return chunked_list\n",
    "\n",
    "    for idx in range(len(data['data_string'])):\n",
    "#         print(data['data_string'][idx])\n",
    "#         data['data_category_number'][idx] = data['data_category_number'][idx] - 1\n",
    "        data['data_category_number'][idx] = data['data_category_number'][idx]\n",
    "        if len(str(data['data_string'][idx]).split()) > 500:\n",
    "#             print(True)\n",
    "            tempString = data['data_string'][idx]\n",
    "            tempStringSplit = tempString.split()\n",
    "            chunkedLists = split_sentences(tempStringSplit)\n",
    "        \n",
    "            for sentence in chunkedLists:\n",
    "                tempSentence = \" \".join(sentence)\n",
    "                data.loc[len(data.index)] = [\n",
    "                                            data['data_id'][idx], \n",
    "                                             tempSentence,\n",
    "                                            data['2d_coor'][idx],\n",
    "                                            data['data_title'][idx],\n",
    "                                            data['data_category'][idx],\n",
    "                                            data['data_category_number'][idx],\n",
    "                                            ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d43e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModel(df, number_of_labels, number_of_epochs=3):\n",
    "    from datasets import load_dataset, Dataset\n",
    "#     dataset = load_dataset('csv', data_files='Care_Reviews.csv', split='train')\n",
    "    df = df.dropna()\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    from datasets import DatasetDict\n",
    "    \n",
    "    train_testvalid = dataset.train_test_split()\n",
    "    test_valid = train_testvalid['test'].train_test_split()\n",
    "    \n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "    \n",
    "    dataset = train_test_valid_dataset.remove_columns(['data_id', '2d_coor', 'data_title','data_category'])\n",
    "    print(dataset)\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "    # refer documentation: padding=True or padding=\"max_length\"\n",
    "        return tokenizer(examples[\"data_string\"], padding=True, truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"valid\"]\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    \n",
    "#     tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"data_category_number\", \"labels\")\n",
    "    \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    small_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "    print(small_train_dataset)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=4)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=4)\n",
    "    \n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=number_of_labels)\n",
    "    \n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from transformers import get_scheduler\n",
    "\n",
    "    num_epochs = number_of_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            print(batch)\n",
    "            outputs = model(**batch)\n",
    "#             print(outputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            break\n",
    "        break\n",
    "            \n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    final_score = metric.compute()\n",
    "    return model, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a456915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_file(data, model, output_file_name=\"something.json\"):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    embedding_list = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        if type(data['data_string'][idx]) is float:\n",
    "            continue\n",
    "        embed = getEmbeddings([data['data_string'][idx]], model, tokenizer).tolist()[0]\n",
    "        embedding_list.append(embed)\n",
    "    \n",
    "    import numpy as np\n",
    "    embeddings_for_umap = np.array(embedding_list)\n",
    "    \n",
    "    import umap.umap_ as umap\n",
    "    umap_embedding = umap.UMAP().fit_transform(embeddings_for_umap, y=list(data['data_category_number']))\n",
    "    \n",
    "    data['2d_coor'] = umap_embedding.tolist()\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "    #     tmp_var = data_df_china_news['2d_coor'][idx].strip('][').split(', ')\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "    #     tmp_dict[\"data_x\"] = str(tmp_var[0])\n",
    "    #     tmp_dict[\"data_y\"] = str(tmp_var[1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "#         tmp_dict[\"data_string\"] = str(data['data_string'][idx])\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b5a00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_masked_bert(data, num_epochs=2, number_of_labels=5):\n",
    "    from transformers import AutoTokenizer, BertForMaskedLM\n",
    "    import torch\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    data = data.dropna()\n",
    "    display(data)\n",
    "    inputs = tokenizer(list(data['data_string']), return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    \n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    \n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "        \n",
    "    class CustomDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings.input_ids)\n",
    "        \n",
    "    dataset = CustomDataset(inputs)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    # and move our model over to the selected device\n",
    "    model.to(device)\n",
    "    # activate training mode\n",
    "    model.train()\n",
    "    \n",
    "    from transformers import AdamW\n",
    "    # initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "    epochs = num_epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            torch.cuda.empty_cache() #############\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            torch.cuda.empty_cache()\n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "    model.save_pretrained('pytorch_model_unsupervised_finetuned')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afdffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneModelUnsupervised(df, number_of_labels=19, number_of_epochs=3):\n",
    "    from datasets import load_dataset, Dataset\n",
    "#     dataset = load_dataset('csv', data_files='Care_Reviews.csv', split='train')\n",
    "    df = df.dropna()\n",
    "    display(df)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    from datasets import DatasetDict\n",
    "    \n",
    "    train_testvalid = dataset.train_test_split()\n",
    "    test_valid = train_testvalid['test'].train_test_split()\n",
    "    \n",
    "    train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "    \n",
    "    dataset = train_test_valid_dataset.remove_columns(['data_id', '2d_coor', 'data_title','data_category'])\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "    # refer documentation: padding=True or padding=\"max_length\"\n",
    "        return tokenizer(examples[\"data_string\"], padding=True, truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"valid\"]\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    \n",
    "#     tokenized_datasets = tokenized_datasets.remove_columns([\"data_string\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"data_category_number\", \"labels\")\n",
    "    \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    \n",
    "    small_train_dataset = tokenized_datasets[\"train\"]\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"]\n",
    "#     print(small_eval_dataset)\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "    eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
    "    \n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"pytorch_model_unsupervised_finetuned\", num_labels=number_of_labels)\n",
    "    \n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    from transformers import get_scheduler\n",
    "\n",
    "    num_epochs = number_of_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "#             print(outputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "#             break\n",
    "#         break\n",
    "            \n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    final_score = metric.compute()\n",
    "    return model, final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are functions to delete files and directories (you will not be able to delete directories/files\n",
    "# directly from the Jupyter Notebook UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30839e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5a071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the space below to play (call) with functions initialized above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bb833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c702f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24523fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dc25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce7fb5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0669d71c-f596-4183-a6ef-6ceb660cac3f</td>\n",
       "      <td>I needed a prescription and thought going to t...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Cleveland Clinic Express Care Clinic</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>7a73d727-b0eb-48e2-8c01-b6faf07ea776</td>\n",
       "      <td>Excellent, friendly staff. Got me in and out i...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - Kenwood</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>baf90288-9dc4-4de4-9b8f-f31322f48747</td>\n",
       "      <td>Awesome staff!!</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - Kenwood</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>672cf0b1-0bc6-45aa-be18-b80d196870f7</td>\n",
       "      <td>I'm really glad that CareFirst open this facil...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - Kenwood</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>26cd97c2-0fea-416a-b5bc-230e8e5daa25</td>\n",
       "      <td>Staff were great and polite</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - Kenwood</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   data_id  \\\n",
       "0     a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1     01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2     1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3     9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4     77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                    ...   \n",
       "9995  0669d71c-f596-4183-a6ef-6ceb660cac3f   \n",
       "9996  7a73d727-b0eb-48e2-8c01-b6faf07ea776   \n",
       "9997  baf90288-9dc4-4de4-9b8f-f31322f48747   \n",
       "9998  672cf0b1-0bc6-45aa-be18-b80d196870f7   \n",
       "9999  26cd97c2-0fea-416a-b5bc-230e8e5daa25   \n",
       "\n",
       "                                            data_string 2d_coor  \\\n",
       "0                                   Good place to visit           \n",
       "1     Went here for a swollen Jaw. Even though I was...           \n",
       "2     I was seen relatively quickly and the staff wa...           \n",
       "3     Reception and service couldn't have been more ...           \n",
       "4     I came in they were very busy the receptionist...           \n",
       "...                                                 ...     ...   \n",
       "9995  I needed a prescription and thought going to t...           \n",
       "9996  Excellent, friendly staff. Got me in and out i...           \n",
       "9997                                    Awesome staff!!           \n",
       "9998  I'm really glad that CareFirst open this facil...           \n",
       "9999                        Staff were great and polite           \n",
       "\n",
       "                                             data_title data_category  \\\n",
       "0     urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1     urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2     urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3     urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4     urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                 ...           ...   \n",
       "9995   urgentCare: Cleveland Clinic Express Care Clinic          Poor   \n",
       "9996        urgentCare: CareFirst Urgent Care - Kenwood      Excelent   \n",
       "9997        urgentCare: CareFirst Urgent Care - Kenwood      Excelent   \n",
       "9998        urgentCare: CareFirst Urgent Care - Kenwood      Excelent   \n",
       "9999        urgentCare: CareFirst Urgent Care - Kenwood      Excelent   \n",
       "\n",
       "     data_category_number  \n",
       "0                       4  \n",
       "1                       1  \n",
       "2                       5  \n",
       "3                       5  \n",
       "4                       5  \n",
       "...                   ...  \n",
       "9995                    1  \n",
       "9996                    5  \n",
       "9997                    5  \n",
       "9998                    5  \n",
       "9999                    5  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert .json to .csv\n",
    "import pandas as pd\n",
    "with open('Care_reviews_10k.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "df\n",
    "df = df.drop(columns=[1,3,4,6])\n",
    "df = df.rename(columns={0:'data_id', 2:'data_title', 5:'data_string'})\n",
    "\n",
    "df['2d_coor'] = ''\n",
    "df['data_category'] = ''\n",
    "df['data_category_number'] = ''\n",
    "df = df.reindex(columns=['data_id', 'data_string', '2d_coor', 'data_title', 'data_category', 'data_category_number'])\n",
    "df.dropna()\n",
    "\n",
    "for index in range(len(df)):\n",
    "    pos=df.loc[index, \"data_title\"].find(\"rating: \")\n",
    "    #print(pos)\n",
    "   \n",
    "    #print(aaa)\n",
    "    df['data_category_number'][index]=df.loc[index, \"data_title\"][pos+8]\n",
    "    if df.loc[index, \"data_title\"][pos+8] == \"1\":\n",
    "        df['data_category'][index]=\"Poor\"\n",
    "    elif df.loc[index, \"data_title\"][pos+8] == \"2\":\n",
    "        df['data_category'][index]=\"Fair\"\n",
    "    elif df.loc[index, \"data_title\"][pos+8] == \"3\":\n",
    "        df['data_category'][index]=\"Good\"\n",
    "    elif df.loc[index, \"data_title\"][pos+8] == \"4\":\n",
    "        df['data_category'][index]=\"Very Good\"\n",
    "    elif df.loc[index, \"data_title\"][pos+8] == \"5\":\n",
    "        df['data_category'][index]=\"Excelent\"\n",
    "    df.loc[index, \"data_title\"]=df.loc[index, \"data_title\"][0:pos-2]\n",
    "df.head()\n",
    "\n",
    "df.to_csv('Care_review_full.csv', encoding='utf-8', index=False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f233888",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1 ###\n",
    "\n",
    "# This function will create a csv in a format (mostly changing columns names) that we need for training models\n",
    "# The following columns are needed: data_category_number, data_title, data_string, data_category, data_id, 2d_coor\n",
    "\n",
    "# Note: This function will have to be modified according to the need as not all datasets have labels but above\n",
    "# mentioned columns should be there\n",
    "def create_structured_csv(csv_file_name):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    #data = data.drop(columns=['Unnamed: 0', 'year'])\n",
    "    data = data.rename(columns={'seq':'data_category_number', 'title':'data_title', 'abstract':'data_string', 'CODE':'data_category','id':'data_id'})\n",
    "    data['2d_coor'] = ''\n",
    "    data = data.reindex(columns=['data_id', 'data_string', '2d_coor', 'data_title', 'data_category', 'data_category_number'])\n",
    "    data.to_csv('Care_review_full.csv')\n",
    "    return data # data is pd object of a csv\n",
    "\n",
    "csv_file_name = 'Care_review_full.csv'\n",
    "data = create_structured_csv(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17eabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2 ###\n",
    "\n",
    "# Not all the datasets at DSC are labelled. Hence, we need to label some and we use LDA for that\n",
    "def apply_lda_on_dataset(df):\n",
    "    import gensim\n",
    "    from gensim.utils import simple_preprocess\n",
    "    from gensim.parsing.preprocessing import STOPWORDS\n",
    "#     from nltk.stem.porter import *\n",
    "    from gensim import corpora, models\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    np.random.seed(2018)\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Function that preprocesses all text documents before feeding to lda model\n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        i = 0\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(token)\n",
    "        return result\n",
    "    \n",
    "    # the file name should be the one generated from the function above     \n",
    "#     data = pd.read_csv('Care_Reviews.csv')\n",
    "    data = df\n",
    "    documents = data\n",
    "    documents = documents.dropna(subset=['data_string'])\n",
    "    processed_docs = documents['data_string'].map(preprocess)\n",
    "    \n",
    "    # Creates a dictionary from the documents (Note: Here the argument 'preprocessed_docs' is a 'list of lists')\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # Creates a bag_of_words corpus     \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # Creates a tfidf matrix/table required for training\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    # Trains an lda model with tfidf\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "    \n",
    "    for idx in range(len(data['data_string'])):\n",
    "        data_string = data['data_string'][idx]\n",
    "#         print(type(data_string))\n",
    "#         print(dictionary.doc2bow(preprocess(data_string)))\n",
    "#         print(lda_model_tfidf.get_document_topics())\n",
    "        data_string_topic_no = lda_model_tfidf.get_document_topics(dictionary.doc2bow(preprocess(data_string)))[0][0]\n",
    "#         print(data_string_topic_no)\n",
    "        data_string_topic = lda_model_tfidf.print_topic(data_string_topic_no)\n",
    "        data['data_category'][idx] = data_string_topic\n",
    "        data_string_topic_num = lda_model_tfidf.get_document_topics(dictionary.doc2bow(preprocess(data_string)))[0][0]\n",
    "        data['data_category_number'][idx] = data_string_topic_num\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7353465f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23095/1345159822.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category'][idx] = data_string_topic\n",
      "/tmp/ipykernel_23095/1345159822.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data_string_topic_num\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23095/648990361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Care_review_full.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_lda_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_23095/1345159822.py\u001b[0m in \u001b[0;36mapply_lda_on_dataset\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#         print(dictionary.doc2bow(preprocess(data_string)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         print(lda_model_tfidf.get_document_topics())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdata_string_topic_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#         print(data_string_topic_no)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdata_string_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string_topic_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1351\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0;31m# Substituting the value of the optimal phi back into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data= pd.read_csv(\"Care_review_full.csv\")\n",
    "data = apply_lda_on_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a500a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('news_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db8cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is responsible for fine-tuning an existing Bert Model (from huggingface) with a DSC dataset\n",
    "def finetuneBertSeqModelWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=15,\n",
    "                                         number_of_epochs=15,\n",
    "                                         output_file_name=\"First_Care_review_full.json\"):\n",
    "    # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)     \n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Function that returns a fine-tuned model (fine-tuned on DSC dataset) and its score\n",
    "    model, score = fineTuneModel(df=data, number_of_labels=number_of_labels, number_of_epochs=number_of_epochs)\n",
    "    \n",
    "    print('Here\\s the fine-tuned model: ', model)\n",
    "    print('Accuracy of the fine-tuned model on the test dataset is: ', score)\n",
    "    \n",
    "    # Function that returns the dataframe with embeddings (UMAP reduces high dimensional embedding to 2D)     \n",
    "    data_with_embeddings = get_json_file(data, model, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429f2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n",
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/s5r6t0jx06n0jzv1j1d39ns00000gr/T/ipykernel_85958/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>e05ca558-6fae-42bb-8271-8f7427fd22dd</td>\n",
       "      <td>doctor but I think I know the difference betwe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - West Chester</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>dba4b761-92ad-4b74-8fdf-f1b0e12debcb</td>\n",
       "      <td>So my experience started out well this morning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Eastside Urgent Care</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>dba4b761-92ad-4b74-8fdf-f1b0e12debcb</td>\n",
       "      <td>what a waste of an hour drive one way and then...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Eastside Urgent Care</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>9aa30e73-9c55-47b2-a181-b1073b60058f</td>\n",
       "      <td>I have mixed emotions about this On Demand cen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Kettering Health On-Demand Care - ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>9aa30e73-9c55-47b2-a181-b1073b60058f</td>\n",
       "      <td>There was no greeting from the receptionist, n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Kettering Health On-Demand Care - ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10044 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "10039  e05ca558-6fae-42bb-8271-8f7427fd22dd   \n",
       "10040  dba4b761-92ad-4b74-8fdf-f1b0e12debcb   \n",
       "10041  dba4b761-92ad-4b74-8fdf-f1b0e12debcb   \n",
       "10042  9aa30e73-9c55-47b2-a181-b1073b60058f   \n",
       "10043  9aa30e73-9c55-47b2-a181-b1073b60058f   \n",
       "\n",
       "                                             data_string  2d_coor  \\\n",
       "0                                    Good place to visit      NaN   \n",
       "1      Went here for a swollen Jaw. Even though I was...      NaN   \n",
       "2      I was seen relatively quickly and the staff wa...      NaN   \n",
       "3      Reception and service couldn't have been more ...      NaN   \n",
       "4      I came in they were very busy the receptionist...      NaN   \n",
       "...                                                  ...      ...   \n",
       "10039  doctor but I think I know the difference betwe...      NaN   \n",
       "10040  So my experience started out well this morning...      NaN   \n",
       "10041  what a waste of an hour drive one way and then...      NaN   \n",
       "10042  I have mixed emotions about this On Demand cen...      NaN   \n",
       "10043  There was no greeting from the receptionist, n...      NaN   \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "10039   urgentCare: CareFirst Urgent Care - West Chester          Poor   \n",
       "10040                   urgentCare: Eastside Urgent Care          Poor   \n",
       "10041                   urgentCare: Eastside Urgent Care          Poor   \n",
       "10042  urgentCare: Kettering Health On-Demand Care - ...          Good   \n",
       "10043  urgentCare: Kettering Health On-Demand Care - ...          Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "10039                     1  \n",
       "10040                     1  \n",
       "10041                     1  \n",
       "10042                     3  \n",
       "10043                     3  \n",
       "\n",
       "[10044 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>e05ca558-6fae-42bb-8271-8f7427fd22dd</td>\n",
       "      <td>doctor but I think I know the difference betwe...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: CareFirst Urgent Care - West Chester</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>dba4b761-92ad-4b74-8fdf-f1b0e12debcb</td>\n",
       "      <td>So my experience started out well this morning...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Eastside Urgent Care</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>dba4b761-92ad-4b74-8fdf-f1b0e12debcb</td>\n",
       "      <td>what a waste of an hour drive one way and then...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Eastside Urgent Care</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>9aa30e73-9c55-47b2-a181-b1073b60058f</td>\n",
       "      <td>I have mixed emotions about this On Demand cen...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Kettering Health On-Demand Care - ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>9aa30e73-9c55-47b2-a181-b1073b60058f</td>\n",
       "      <td>There was no greeting from the receptionist, n...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Kettering Health On-Demand Care - ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10044 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "10039  e05ca558-6fae-42bb-8271-8f7427fd22dd   \n",
       "10040  dba4b761-92ad-4b74-8fdf-f1b0e12debcb   \n",
       "10041  dba4b761-92ad-4b74-8fdf-f1b0e12debcb   \n",
       "10042  9aa30e73-9c55-47b2-a181-b1073b60058f   \n",
       "10043  9aa30e73-9c55-47b2-a181-b1073b60058f   \n",
       "\n",
       "                                             data_string 2d_coor  \\\n",
       "0                                    Good place to visit           \n",
       "1      Went here for a swollen Jaw. Even though I was...           \n",
       "2      I was seen relatively quickly and the staff wa...           \n",
       "3      Reception and service couldn't have been more ...           \n",
       "4      I came in they were very busy the receptionist...           \n",
       "...                                                  ...     ...   \n",
       "10039  doctor but I think I know the difference betwe...           \n",
       "10040  So my experience started out well this morning...           \n",
       "10041  what a waste of an hour drive one way and then...           \n",
       "10042  I have mixed emotions about this On Demand cen...           \n",
       "10043  There was no greeting from the receptionist, n...           \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "10039   urgentCare: CareFirst Urgent Care - West Chester          Poor   \n",
       "10040                   urgentCare: Eastside Urgent Care          Poor   \n",
       "10041                   urgentCare: Eastside Urgent Care          Poor   \n",
       "10042  urgentCare: Kettering Health On-Demand Care - ...          Good   \n",
       "10043  urgentCare: Kettering Health On-Demand Care - ...          Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "10039                     1  \n",
       "10040                     1  \n",
       "10041                     1  \n",
       "10042                     3  \n",
       "10043                     3  \n",
       "\n",
       "[10044 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ez/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parameter 'function'=<function fineTuneModel.<locals>.tokenize_function at 0x146d98430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 10.12ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.92ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  9.12ba/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                     | 28/28260 [01:32<25:53:27,  3.30s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinetuneBertSeqModelWithCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCare_review_full.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFirst_Care_review_full.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mfinetuneBertSeqModelWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, number_of_labels, number_of_epochs, output_file_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m display(data)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Function that returns a fine-tuned model (fine-tuned on DSC dataset) and its score\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model, score \u001b[38;5;241m=\u001b[39m \u001b[43mfineTuneModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHere\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms the fine-tuned model: \u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the fine-tuned model on the test dataset is: \u001b[39m\u001b[38;5;124m'\u001b[39m, score)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mfineTuneModel\u001b[0;34m(df, number_of_labels, number_of_epochs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#             print(outputs)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m             loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 80\u001b[0m             \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m             lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetuneBertSeqModelWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=15,\n",
    "                                         number_of_epochs=15,\n",
    "                                         output_file_name=\"First_Care_review_full.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f66c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is responsible for pretraining a masked bert model and fine-tuning the same pretrained model \n",
    "# (from huggingface) with a DSC dataset\n",
    "def finetuneBertModelAfterPretrainingOfMaskedBertWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=19,\n",
    "                                         number_of_epochs_for_masked_bert=2,\n",
    "                                         number_of_epochs_for_finetuning_masked_bert=5,                 \n",
    "                                         output_file_name=\"Second_Care_review_full.json\"):\n",
    "    \n",
    "    # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Function that pretrains a masked bert model and saves that model in a directory: 'pytorch_model_unsupervised_finetuned'\n",
    "    train_masked_bert(data, num_epochs=number_of_epochs_for_masked_bert, number_of_labels=number_of_labels)\n",
    "    \n",
    "    # Function that fine-tunes the above pretrained masked bert model\n",
    "    new_model, score = fineTuneModelUnsupervised(df, number_of_labels=number_of_labels, number_of_epochs=number_of_epochs_for_finetuning_masked_bert)\n",
    "    \n",
    "    print('Here\\s the fine-tuned model: ', model)\n",
    "    print('Accuracy of the fine-tuned model on the test dataset is: ', score)\n",
    "    \n",
    "    # Function that returns the dataframe with embeddings (UMAP reduces high dimensional embedding to 2D)     \n",
    "    data_with_embeddings = get_json_file(data, model, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3453051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23095/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33491</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>anyways the whole experience was awful and I h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33492</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33493</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>ER. The doctors don't listen or care about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33494</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33495</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>needed something at once, my nurses bopped the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33491  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33492  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33493  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33494  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33495  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string  2d_coor  \\\n",
       "0                                    Good place to visit      NaN   \n",
       "1      Went here for a swollen Jaw. Even though I was...      NaN   \n",
       "2      I was seen relatively quickly and the staff wa...      NaN   \n",
       "3      Reception and service couldn't have been more ...      NaN   \n",
       "4      I came in they were very busy the receptionist...      NaN   \n",
       "...                                                  ...      ...   \n",
       "33491  anyways the whole experience was awful and I h...      NaN   \n",
       "33492  went to this ER twice and each time the doctor...      NaN   \n",
       "33493  ER. The doctors don't listen or care about the...      NaN   \n",
       "33494  I usually review restaurants, but occasionally...      NaN   \n",
       "33495  needed something at once, my nurses bopped the...      NaN   \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33491                hospital: Licking Memorial Hospital          Poor   \n",
       "33492                        hospital: Mount Carmel East          Poor   \n",
       "33493                        hospital: Mount Carmel East          Poor   \n",
       "33494  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33495  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33491                     1  \n",
       "33492                     1  \n",
       "33493                     1  \n",
       "33494                     4  \n",
       "33495                     4  \n",
       "\n",
       "[33496 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33491</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>anyways the whole experience was awful and I h...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33492</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33493</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>ER. The doctors don't listen or care about the...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33494</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33495</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>needed something at once, my nurses bopped the...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33491  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33492  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33493  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33494  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33495  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string 2d_coor  \\\n",
       "0                                    Good place to visit           \n",
       "1      Went here for a swollen Jaw. Even though I was...           \n",
       "2      I was seen relatively quickly and the staff wa...           \n",
       "3      Reception and service couldn't have been more ...           \n",
       "4      I came in they were very busy the receptionist...           \n",
       "...                                                  ...     ...   \n",
       "33491  anyways the whole experience was awful and I h...           \n",
       "33492  went to this ER twice and each time the doctor...           \n",
       "33493  ER. The doctors don't listen or care about the...           \n",
       "33494  I usually review restaurants, but occasionally...           \n",
       "33495  needed something at once, my nurses bopped the...           \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33491                hospital: Licking Memorial Hospital          Poor   \n",
       "33492                        hospital: Mount Carmel East          Poor   \n",
       "33493                        hospital: Mount Carmel East          Poor   \n",
       "33494  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33495  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33491                     1  \n",
       "33492                     1  \n",
       "33493                     1  \n",
       "33494                     4  \n",
       "33495                     4  \n",
       "\n",
       "[33496 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33491</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>anyways the whole experience was awful and I h...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33492</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33493</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>ER. The doctors don't listen or care about the...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33494</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33495</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>needed something at once, my nurses bopped the...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33491  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33492  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33493  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33494  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33495  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string 2d_coor  \\\n",
       "0                                    Good place to visit           \n",
       "1      Went here for a swollen Jaw. Even though I was...           \n",
       "2      I was seen relatively quickly and the staff wa...           \n",
       "3      Reception and service couldn't have been more ...           \n",
       "4      I came in they were very busy the receptionist...           \n",
       "...                                                  ...     ...   \n",
       "33491  anyways the whole experience was awful and I h...           \n",
       "33492  went to this ER twice and each time the doctor...           \n",
       "33493  ER. The doctors don't listen or care about the...           \n",
       "33494  I usually review restaurants, but occasionally...           \n",
       "33495  needed something at once, my nurses bopped the...           \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33491                hospital: Licking Memorial Hospital          Poor   \n",
       "33492                        hospital: Mount Carmel East          Poor   \n",
       "33493                        hospital: Mount Carmel East          Poor   \n",
       "33494  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33495  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33491                     1  \n",
       "33492                     1  \n",
       "33493                     1  \n",
       "33494                     4  \n",
       "33495                     4  \n",
       "\n",
       "[33496 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23095/4062186638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m finetuneBertModelAfterPretrainingOfMaskedBertWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n\u001b[0m\u001b[1;32m      2\u001b[0m                                          \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0mnumber_of_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          \u001b[0mnumber_of_epochs_for_masked_bert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0mnumber_of_epochs_for_finetuning_masked_bert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23095/261647460.py\u001b[0m in \u001b[0;36mfinetuneBertModelAfterPretrainingOfMaskedBertWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, number_of_labels, number_of_epochs_for_masked_bert, number_of_epochs_for_finetuning_masked_bert, output_file_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Function that pretrains a masked bert model and saves that model in a directory: 'pytorch_model_unsupervised_finetuned'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_masked_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_epochs_for_masked_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Function that fine-tunes the above pretrained masked bert model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23095/3276481884.py\u001b[0m in \u001b[0;36mtrain_masked_bert\u001b[0;34m(data, num_epochs, number_of_labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# device = torch.device('cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# and move our model over to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;31m# activate training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "finetuneBertModelAfterPretrainingOfMaskedBertWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                                         model_max_length=500,\n",
    "                                         number_of_labels=19,\n",
    "                                         number_of_epochs_for_masked_bert=2,\n",
    "                                         number_of_epochs_for_finetuning_masked_bert=5,                 \n",
    "                                         output_file_name=\"Second_Care_review_full.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ed93b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UMAPWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='Third_Care_review_full.json',\n",
    "                         use_labels=True,\n",
    "                         sentence_transformer_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_umap = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with UMAP\n",
    "    import umap.umap_ as umap\n",
    "    umap_embedding = umap.UMAP().fit_transform(embeddings_for_umap, y=list(data['data_category_number']) if use_labels else None)\n",
    "    \n",
    "    data['2d_coor'] = umap_embedding.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbb8662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23095/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33498</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>and asked me like a hundred ridiculous questio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33499</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33500</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>TO HELP ME. Why would you work in the medical ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33501</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33502</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>60s/70s. The hospital had wisely turned off th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33498  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33499  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33500  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33501  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33502  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string  2d_coor  \\\n",
       "0                                    Good place to visit      NaN   \n",
       "1      Went here for a swollen Jaw. Even though I was...      NaN   \n",
       "2      I was seen relatively quickly and the staff wa...      NaN   \n",
       "3      Reception and service couldn't have been more ...      NaN   \n",
       "4      I came in they were very busy the receptionist...      NaN   \n",
       "...                                                  ...      ...   \n",
       "33498  and asked me like a hundred ridiculous questio...      NaN   \n",
       "33499  went to this ER twice and each time the doctor...      NaN   \n",
       "33500  TO HELP ME. Why would you work in the medical ...      NaN   \n",
       "33501  I usually review restaurants, but occasionally...      NaN   \n",
       "33502  60s/70s. The hospital had wisely turned off th...      NaN   \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33498                hospital: Licking Memorial Hospital          Poor   \n",
       "33499                        hospital: Mount Carmel East          Poor   \n",
       "33500                        hospital: Mount Carmel East          Poor   \n",
       "33501  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33502  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33498                     1  \n",
       "33499                     1  \n",
       "33500                     1  \n",
       "33501                     4  \n",
       "33502                     4  \n",
       "\n",
       "[33503 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33498</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>and asked me like a hundred ridiculous questio...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33499</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33500</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>TO HELP ME. Why would you work in the medical ...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33501</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33502</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>60s/70s. The hospital had wisely turned off th...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33498  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33499  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33500  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33501  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33502  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string 2d_coor  \\\n",
       "0                                    Good place to visit           \n",
       "1      Went here for a swollen Jaw. Even though I was...           \n",
       "2      I was seen relatively quickly and the staff wa...           \n",
       "3      Reception and service couldn't have been more ...           \n",
       "4      I came in they were very busy the receptionist...           \n",
       "...                                                  ...     ...   \n",
       "33498  and asked me like a hundred ridiculous questio...           \n",
       "33499  went to this ER twice and each time the doctor...           \n",
       "33500  TO HELP ME. Why would you work in the medical ...           \n",
       "33501  I usually review restaurants, but occasionally...           \n",
       "33502  60s/70s. The hospital had wisely turned off th...           \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33498                hospital: Licking Memorial Hospital          Poor   \n",
       "33499                        hospital: Mount Carmel East          Poor   \n",
       "33500                        hospital: Mount Carmel East          Poor   \n",
       "33501  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33502  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33498                     1  \n",
       "33499                     1  \n",
       "33500                     1  \n",
       "33501                     4  \n",
       "33502                     4  \n",
       "\n",
       "[33503 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23095/230279547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m UMAPWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0moutput_json_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Third_Care_review_full.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0muse_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          sentence_transformer_name='sentence-transformers/all-mpnet-base-v2')\n",
      "\u001b[0;32m/tmp/ipykernel_23095/718799129.py\u001b[0m in \u001b[0;36mUMAPWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, output_json_file_name, use_labels, sentence_transformer_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_transformer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_string'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0membeddings_for_umap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Reducing the dimensionality of embeddings with UMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "UMAPWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='Third_Care_review_full.json',\n",
    "                         use_labels=True,\n",
    "                         sentence_transformer_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f293c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3355/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...      NaN   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...      NaN   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...      NaN   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...      NaN   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...      NaN   \n",
       "...         ...                                                ...      ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...      NaN   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...      NaN   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...      NaN   \n",
       "193490   813146  work of experts, including independent auditor...      NaN   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...      NaN   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td></td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td></td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td></td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td></td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td></td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string 2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...           \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...           \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...           \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...           \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...           \n",
       "...         ...                                                ...     ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...           \n",
       "193488   813146  Individually or Collectively, Lead to Negative...           \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...           \n",
       "193490   813146  work of experts, including independent auditor...           \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...           \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[6.541574001312256, 5.994854927062988]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.31491324305534363, 5.71586799621582]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[0.19933286309242249, 8.378273963928223]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[6.3932318687438965, 10.417623519897461]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[7.935702800750732, -7.721569061279297]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[3.2640509605407715, 16.858848571777344]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[3.2133913040161133, 16.883222579956055]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[2.730613946914673, 17.219083786010742]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[9.606926918029785, 17.593767166137695]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[15.867743492126465, -4.674909591674805]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                         2d_coor  \\\n",
       "0         [6.541574001312256, 5.994854927062988]   \n",
       "1       [-0.31491324305534363, 5.71586799621582]   \n",
       "2       [0.19933286309242249, 8.378273963928223]   \n",
       "3       [6.3932318687438965, 10.417623519897461]   \n",
       "4        [7.935702800750732, -7.721569061279297]   \n",
       "...                                          ...   \n",
       "193487  [3.2640509605407715, 16.858848571777344]   \n",
       "193488  [3.2133913040161133, 16.883222579956055]   \n",
       "193489   [2.730613946914673, 17.219083786010742]   \n",
       "193490   [9.606926918029785, 17.593767166137695]   \n",
       "193491  [15.867743492126465, -4.674909591674805]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[6.541574001312256, 5.994854927062988]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.31491324305534363, 5.71586799621582]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[0.19933286309242249, 8.378273963928223]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[6.3932318687438965, 10.417623519897461]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[7.935702800750732, -7.721569061279297]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[3.2640509605407715, 16.858848571777344]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[3.2133913040161133, 16.883222579956055]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[2.730613946914673, 17.219083786010742]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[9.606926918029785, 17.593767166137695]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[15.867743492126465, -4.674909591674805]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                         2d_coor  \\\n",
       "0         [6.541574001312256, 5.994854927062988]   \n",
       "1       [-0.31491324305534363, 5.71586799621582]   \n",
       "2       [0.19933286309242249, 8.378273963928223]   \n",
       "3       [6.3932318687438965, 10.417623519897461]   \n",
       "4        [7.935702800750732, -7.721569061279297]   \n",
       "...                                          ...   \n",
       "193487  [3.2640509605407715, 16.858848571777344]   \n",
       "193488  [3.2133913040161133, 16.883222579956055]   \n",
       "193489   [2.730613946914673, 17.219083786010742]   \n",
       "193490   [9.606926918029785, 17.593767166137695]   \n",
       "193491  [15.867743492126465, -4.674909591674805]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = UMAPWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_umap.json',\n",
    "                         use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11f50df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                         use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_tsne = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with TSNE\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(embeddings_for_tsne, list(data['data_category_number']) if use_labels else None)\n",
    "    \n",
    "    data['2d_coor'] = tsne_results.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58445cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3355/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...      NaN   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...      NaN   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...      NaN   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...      NaN   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...      NaN   \n",
       "...         ...                                                ...      ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...      NaN   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...      NaN   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...      NaN   \n",
       "193490   813146  work of experts, including independent auditor...      NaN   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...      NaN   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td></td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td></td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td></td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td></td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td></td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string 2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...           \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...           \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...           \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...           \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...           \n",
       "...         ...                                                ...     ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...           \n",
       "193488   813146  Individually or Collectively, Lead to Negative...           \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...           \n",
       "193490   813146  work of experts, including independent auditor...           \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...           \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 193492 samples in 0.069s...\n",
      "[t-SNE] Computed neighbors for 193492 samples in 1197.803s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 101000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 102000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 103000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 104000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 105000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 106000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 107000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 108000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 109000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 110000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 111000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 112000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 113000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 114000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 115000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 116000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 117000 / 193492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 118000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 119000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 120000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 121000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 122000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 123000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 124000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 125000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 126000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 127000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 128000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 129000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 130000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 131000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 132000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 133000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 134000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 135000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 136000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 137000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 138000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 139000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 140000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 141000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 142000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 143000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 144000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 145000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 146000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 147000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 148000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 149000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 150000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 151000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 152000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 153000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 154000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 155000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 156000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 157000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 158000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 159000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 160000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 161000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 162000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 163000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 164000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 165000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 166000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 167000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 168000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 169000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 170000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 171000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 172000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 173000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 174000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 175000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 176000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 177000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 178000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 179000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 180000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 181000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 182000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 183000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 184000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 185000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 186000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 187000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 188000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 189000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 190000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 191000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 192000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 193000 / 193492\n",
      "[t-SNE] Computed conditional probabilities for sample 193492 / 193492\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 50 iterations with early exaggeration: 128.080566\n",
      "[t-SNE] KL divergence after 300 iterations: 5.640841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[4.397582054138184, -3.5684354305267334]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.3279964327812195, 8.61110782623291]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[4.169193744659424, -4.005739212036133]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[-2.864100933074951, -0.9488449692726135]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[0.2897432744503021, 10.175537109375]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[-5.589747428894043, 3.172562599182129]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[4.083289623260498, 9.9985933303833]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[4.8006367683410645, 9.934613227844238]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[4.684049606323242, 9.922048568725586]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[12.157347679138184, 0.8815428614616394]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                          2d_coor  \\\n",
       "0        [4.397582054138184, -3.5684354305267334]   \n",
       "1         [-0.3279964327812195, 8.61110782623291]   \n",
       "2         [4.169193744659424, -4.005739212036133]   \n",
       "3       [-2.864100933074951, -0.9488449692726135]   \n",
       "4           [0.2897432744503021, 10.175537109375]   \n",
       "...                                           ...   \n",
       "193487    [-5.589747428894043, 3.172562599182129]   \n",
       "193488       [4.083289623260498, 9.9985933303833]   \n",
       "193489    [4.8006367683410645, 9.934613227844238]   \n",
       "193490     [4.684049606323242, 9.922048568725586]   \n",
       "193491   [12.157347679138184, 0.8815428614616394]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[4.397582054138184, -3.5684354305267334]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[-0.3279964327812195, 8.61110782623291]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[4.169193744659424, -4.005739212036133]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[-2.864100933074951, -0.9488449692726135]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[0.2897432744503021, 10.175537109375]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[-5.589747428894043, 3.172562599182129]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[4.083289623260498, 9.9985933303833]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[4.8006367683410645, 9.934613227844238]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[4.684049606323242, 9.922048568725586]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[12.157347679138184, 0.8815428614616394]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                          2d_coor  \\\n",
       "0        [4.397582054138184, -3.5684354305267334]   \n",
       "1         [-0.3279964327812195, 8.61110782623291]   \n",
       "2         [4.169193744659424, -4.005739212036133]   \n",
       "3       [-2.864100933074951, -0.9488449692726135]   \n",
       "4           [0.2897432744503021, 10.175537109375]   \n",
       "...                                           ...   \n",
       "193487    [-5.589747428894043, 3.172562599182129]   \n",
       "193488       [4.083289623260498, 9.9985933303833]   \n",
       "193489    [4.8006367683410645, 9.934613227844238]   \n",
       "193490     [4.684049606323242, 9.922048568725586]   \n",
       "193491   [12.157347679138184, 0.8815428614616394]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = TSNEWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_tsne.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                         use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "317202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_pca = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_pca, list(data['data_category_number']) if use_labels else None)\n",
    "\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_pca)\n",
    "    \n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    \n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "        tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13f93324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3355/1359901329.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...      NaN   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...      NaN   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...      NaN   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...      NaN   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...      NaN   \n",
       "...         ...                                                ...      ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...      NaN   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...      NaN   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...      NaN   \n",
       "193490   813146  work of experts, including independent auditor...      NaN   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...      NaN   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td></td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td></td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td></td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td></td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td></td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td></td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string 2d_coor  \\\n",
       "0           115  A collaboration between artist Christina Kelly...           \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...           \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...           \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...           \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...           \n",
       "...         ...                                                ...     ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...           \n",
       "193488   813146  Individually or Collectively, Lead to Negative...           \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...           \n",
       "193490   813146  work of experts, including independent auditor...           \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...           \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[-0.21102119982242584, -0.1436595618724823]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[0.12903806567192078, 0.03234309330582619]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[-0.14857056736946106, -0.17932890355587006]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[0.33694982528686523, -0.17755252122879028]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[-0.026972472667694092, 0.012225701473653316]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[0.3518408536911011, -0.1886298656463623]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[0.31185394525527954, -0.04102354869246483]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[0.1637495756149292, 0.08634164929389954]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[0.14689122140407562, 0.08248250186443329]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[0.2927764058113098, -0.1037045493721962]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                              2d_coor  \\\n",
       "0         [-0.21102119982242584, -0.1436595618724823]   \n",
       "1          [0.12903806567192078, 0.03234309330582619]   \n",
       "2        [-0.14857056736946106, -0.17932890355587006]   \n",
       "3         [0.33694982528686523, -0.17755252122879028]   \n",
       "4       [-0.026972472667694092, 0.012225701473653316]   \n",
       "...                                               ...   \n",
       "193487      [0.3518408536911011, -0.1886298656463623]   \n",
       "193488    [0.31185394525527954, -0.04102354869246483]   \n",
       "193489      [0.1637495756149292, 0.08634164929389954]   \n",
       "193490     [0.14689122140407562, 0.08248250186443329]   \n",
       "193491      [0.2927764058113098, -0.1037045493721962]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>A collaboration between artist Christina Kelly...</td>\n",
       "      <td>[-0.21102119982242584, -0.1436595618724823]</td>\n",
       "      <td>The History of Gowanus Cemented in Sculpture</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>As Hurricane Irma draws closer to the Florida ...</td>\n",
       "      <td>[0.12903806567192078, 0.03234309330582619]</td>\n",
       "      <td>Emergency Services Rush to Save Expensive Wine...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>Raúl Ortega Ayala’s new exhibition at Proyecto...</td>\n",
       "      <td>[-0.14857056736946106, -0.17932890355587006]</td>\n",
       "      <td>An Artist Serves Up Food for Thought About Exc...</td>\n",
       "      <td>0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>Welcome to the public markets, Snapchat. Stock...</td>\n",
       "      <td>[0.33694982528686523, -0.17755252122879028]</td>\n",
       "      <td>Snap stock took a beating Monday and fell more...</td>\n",
       "      <td>0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>Vox Sentences is written by Dylan Matthews and...</td>\n",
       "      <td>[-0.026972472667694092, 0.012225701473653316]</td>\n",
       "      <td>Vox Sentences: There’s a coup underway in Turkey</td>\n",
       "      <td>0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193487</th>\n",
       "      <td>813146</td>\n",
       "      <td>VNO's case approximately 90% of EBITDA will be...</td>\n",
       "      <td>[0.3518408536911011, -0.1886298656463623]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193488</th>\n",
       "      <td>813146</td>\n",
       "      <td>Individually or Collectively, Lead to Negative...</td>\n",
       "      <td>[0.31185394525527954, -0.04102354869246483]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193489</th>\n",
       "      <td>813146</td>\n",
       "      <td>THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...</td>\n",
       "      <td>[0.1637495756149292, 0.08634164929389954]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193490</th>\n",
       "      <td>813146</td>\n",
       "      <td>work of experts, including independent auditor...</td>\n",
       "      <td>[0.14689122140407562, 0.08248250186443329]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193491</th>\n",
       "      <td>813146</td>\n",
       "      <td>to US$750,000 (or the applicable currency equi...</td>\n",
       "      <td>[0.2927764058113098, -0.1037045493721962]</td>\n",
       "      <td>Fitch Affirms American Assets Trust's IDR at '...</td>\n",
       "      <td>0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193492 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_id                                        data_string  \\\n",
       "0           115  A collaboration between artist Christina Kelly...   \n",
       "1           118  As Hurricane Irma draws closer to the Florida ...   \n",
       "2           119  Raúl Ortega Ayala’s new exhibition at Proyecto...   \n",
       "3           122  Welcome to the public markets, Snapchat. Stock...   \n",
       "4           125  Vox Sentences is written by Dylan Matthews and...   \n",
       "...         ...                                                ...   \n",
       "193487   813146  VNO's case approximately 90% of EBITDA will be...   \n",
       "193488   813146  Individually or Collectively, Lead to Negative...   \n",
       "193489   813146  THIS SITE. DIRECTORS AND SHAREHOLDERS RELEVANT...   \n",
       "193490   813146  work of experts, including independent auditor...   \n",
       "193491   813146  to US$750,000 (or the applicable currency equi...   \n",
       "\n",
       "                                              2d_coor  \\\n",
       "0         [-0.21102119982242584, -0.1436595618724823]   \n",
       "1          [0.12903806567192078, 0.03234309330582619]   \n",
       "2        [-0.14857056736946106, -0.17932890355587006]   \n",
       "3         [0.33694982528686523, -0.17755252122879028]   \n",
       "4       [-0.026972472667694092, 0.012225701473653316]   \n",
       "...                                               ...   \n",
       "193487      [0.3518408536911011, -0.1886298656463623]   \n",
       "193488    [0.31185394525527954, -0.04102354869246483]   \n",
       "193489      [0.1637495756149292, 0.08634164929389954]   \n",
       "193490     [0.14689122140407562, 0.08248250186443329]   \n",
       "193491      [0.2927764058113098, -0.1037045493721962]   \n",
       "\n",
       "                                               data_title  \\\n",
       "0            The History of Gowanus Cemented in Sculpture   \n",
       "1       Emergency Services Rush to Save Expensive Wine...   \n",
       "2       An Artist Serves Up Food for Thought About Exc...   \n",
       "3       Snap stock took a beating Monday and fell more...   \n",
       "4        Vox Sentences: There’s a coup underway in Turkey   \n",
       "...                                                   ...   \n",
       "193487  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193488  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193489  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193490  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "193491  Fitch Affirms American Assets Trust's IDR at '...   \n",
       "\n",
       "                                            data_category  \\\n",
       "0       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "1       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "2       0.005*\"colbert\" + 0.005*\"corden\" + 0.004*\"week...   \n",
       "3       0.003*\"apple\" + 0.003*\"facebook\" + 0.002*\"goog...   \n",
       "4       0.005*\"percent\" + 0.004*\"reuters\" + 0.004*\"com...   \n",
       "...                                                   ...   \n",
       "193487  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193488  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193489  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193490  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "193491  0.011*\"million\" + 0.009*\"versus\" + 0.009*\"eiko...   \n",
       "\n",
       "        data_category_number  \n",
       "0                         11  \n",
       "1                         11  \n",
       "2                          7  \n",
       "3                         11  \n",
       "4                         12  \n",
       "...                      ...  \n",
       "193487                     4  \n",
       "193488                     4  \n",
       "193489                     4  \n",
       "193490                     4  \n",
       "193491                     4  \n",
       "\n",
       "[193492 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = PCAWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_pca.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1946061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansAndPCAWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='Fifth_Care_review_full.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_kmeans = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA After applying KMeans\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=len(data['data_category_number'].unique()))\n",
    "    labels = kmeans.fit_predict(embeddings_for_kmeans, list(data['data_category_number']) if use_labels else None)\n",
    "    labels_scale = kmeans.labels_\n",
    "\n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_kmeans, labels.tolist())\n",
    "\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_kmeans)\n",
    "    \n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    new_labels = labels.tolist()\n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "#         tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_category_number\"] = str(new_labels[idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a03b04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23095/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23095/2654660204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m d = KMeansAndPCAWithCustomDataset(input_file_name='news_articles.csv', \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0moutput_json_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news_articles_kmeanspca.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0msentence_transformer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mp_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23095/3293273879.py\u001b[0m in \u001b[0;36mKMeansAndPCAWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, output_json_file_name, sentence_transformer_name, p_components, use_labels)\u001b[0m\n\u001b[1;32m      7\u001b[0m                         use_labels=True):\n\u001b[1;32m      8\u001b[0m      \u001b[0;31m# Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataframe with reduced sentence sizes: \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23095/492771678.py\u001b[0m in \u001b[0;36mupdateDataFrame\u001b[0;34m(csv_file_name, model_max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunkedLists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mtempSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 data.loc[len(data.index)] = [\n\u001b[0m\u001b[1;32m     29\u001b[0m                                             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                              \u001b[0mtempSentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2025\u001b[0m                     \u001b[0;31m# must have conforming columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2027\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "d = KMeansAndPCAWithCustomDataset(input_file_name='news_articles.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='news_articles_kmeanspca.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65bb72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAAndKMeansWithCustomDataset(input_file_name='Care_Reviews.csv', \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name='test.json',\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True):\n",
    "     # Function that returns a new dataframe with reduced sentence sizes (as most bert models have a max_seq_length)\n",
    "    data = updateDataFrame(input_file_name, model_max_length=model_max_length)\n",
    "    print('Dataframe with reduced sentence sizes: \\n')\n",
    "    display(data)\n",
    "    \n",
    "    # Replacing all NaN fields under '2d_coor' column with an empty string\n",
    "    print('Dataframe with NaN removed: \\n')\n",
    "    data['2d_coor'] = ''\n",
    "    display(data)\n",
    "    \n",
    "    # Using a bert model from sentence_transformers to generate embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(sentence_transformer_name)\n",
    "    sentences = data['data_string']\n",
    "    embeddings_for_kmeans = model.encode(sentences)\n",
    "    \n",
    "    # Reducing the dimensionality of embeddings with PCA After applying KMeans\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca_2d_class = PCA(n_components=p_components).fit(embeddings_for_kmeans, list(data['data_category_number']) if use_labels else None)\n",
    "    pca_2d = pca_2d_class.transform(embeddings_for_kmeans)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=len(data['data_category_number'].unique()))\n",
    "    labels = kmeans.fit_predict(pca_2d, list(data['data_category_number']) if use_labels else None)\n",
    "    labels_scale = kmeans.labels_\n",
    "\n",
    "    data['2d_coor'] = pca_2d.tolist()\n",
    "    display(data)\n",
    "    \n",
    "    new_labels = labels.tolist()\n",
    "    list_of_points = []\n",
    "    for idx in range(len(data['data_string'])):\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[\"data_x\"] = str(data['2d_coor'][idx][0])\n",
    "        tmp_dict[\"data_y\"] = str(data['2d_coor'][idx][1])\n",
    "#         tmp_dict[\"data_category_number\"] = str(data['data_category_number'][idx])\n",
    "        tmp_dict[\"data_category_number\"] = str(new_labels[idx])\n",
    "        tmp_dict[\"data_id\"] = str(data['data_id'][idx])\n",
    "\n",
    "        tmp_dict[\"data_title\"] = str(data['data_title'][idx])\n",
    "        tmp_dict[\"data_category\"] = str(data['data_category'][idx])\n",
    "\n",
    "        list_of_points.append(tmp_dict)\n",
    "        \n",
    "    import json\n",
    "    with open(output_json_file_name, \"w\") as outfile:\n",
    "        json.dump(list_of_points, outfile)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97a017f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23095/492771678.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['data_category_number'][idx] = data['data_category_number'][idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with reduced sentence sizes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33498</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>and asked me like a hundred ridiculous questio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33499</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33500</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>TO HELP ME. Why would you work in the medical ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33501</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33502</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>60s/70s. The hospital had wisely turned off th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33498  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33499  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33500  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33501  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33502  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string  2d_coor  \\\n",
       "0                                    Good place to visit      NaN   \n",
       "1      Went here for a swollen Jaw. Even though I was...      NaN   \n",
       "2      I was seen relatively quickly and the staff wa...      NaN   \n",
       "3      Reception and service couldn't have been more ...      NaN   \n",
       "4      I came in they were very busy the receptionist...      NaN   \n",
       "...                                                  ...      ...   \n",
       "33498  and asked me like a hundred ridiculous questio...      NaN   \n",
       "33499  went to this ER twice and each time the doctor...      NaN   \n",
       "33500  TO HELP ME. Why would you work in the medical ...      NaN   \n",
       "33501  I usually review restaurants, but occasionally...      NaN   \n",
       "33502  60s/70s. The hospital had wisely turned off th...      NaN   \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33498                hospital: Licking Memorial Hospital          Poor   \n",
       "33499                        hospital: Mount Carmel East          Poor   \n",
       "33500                        hospital: Mount Carmel East          Poor   \n",
       "33501  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33502  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33498                     1  \n",
       "33499                     1  \n",
       "33500                     1  \n",
       "33501                     4  \n",
       "33502                     4  \n",
       "\n",
       "[33503 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with NaN removed: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_string</th>\n",
       "      <th>2d_coor</th>\n",
       "      <th>data_title</th>\n",
       "      <th>data_category</th>\n",
       "      <th>data_category_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a224545f-1a07-47eb-96ee-a0d0cab23100</td>\n",
       "      <td>Good place to visit</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01a8cfc8-e207-4c72-a9fa-fa6353fde529</td>\n",
       "      <td>Went here for a swollen Jaw. Even though I was...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1a4fb4a6-6c56-4610-a151-b4b25a774cc8</td>\n",
       "      <td>I was seen relatively quickly and the staff wa...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9356a4fd-54fd-4604-a6a8-2d3067eba7cc</td>\n",
       "      <td>Reception and service couldn't have been more ...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77526eed-5ff3-4f2f-80ce-da1048137cf4</td>\n",
       "      <td>I came in they were very busy the receptionist...</td>\n",
       "      <td></td>\n",
       "      <td>urgentCare: Access Urgent Medical Care Pickeri...</td>\n",
       "      <td>Excelent</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33498</th>\n",
       "      <td>37b547d6-931d-417d-b732-95845b0bcc22</td>\n",
       "      <td>and asked me like a hundred ridiculous questio...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Licking Memorial Hospital</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33499</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>went to this ER twice and each time the doctor...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33500</th>\n",
       "      <td>26efef6b-9f07-4a25-8a91-f85926b2012e</td>\n",
       "      <td>TO HELP ME. Why would you work in the medical ...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: Mount Carmel East</td>\n",
       "      <td>Poor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33501</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>I usually review restaurants, but occasionally...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33502</th>\n",
       "      <td>017ced75-7179-4f6f-b467-32113a9c1704</td>\n",
       "      <td>60s/70s. The hospital had wisely turned off th...</td>\n",
       "      <td></td>\n",
       "      <td>hospital: University Hospitals Parma Medical C...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33503 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    data_id  \\\n",
       "0      a224545f-1a07-47eb-96ee-a0d0cab23100   \n",
       "1      01a8cfc8-e207-4c72-a9fa-fa6353fde529   \n",
       "2      1a4fb4a6-6c56-4610-a151-b4b25a774cc8   \n",
       "3      9356a4fd-54fd-4604-a6a8-2d3067eba7cc   \n",
       "4      77526eed-5ff3-4f2f-80ce-da1048137cf4   \n",
       "...                                     ...   \n",
       "33498  37b547d6-931d-417d-b732-95845b0bcc22   \n",
       "33499  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33500  26efef6b-9f07-4a25-8a91-f85926b2012e   \n",
       "33501  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "33502  017ced75-7179-4f6f-b467-32113a9c1704   \n",
       "\n",
       "                                             data_string 2d_coor  \\\n",
       "0                                    Good place to visit           \n",
       "1      Went here for a swollen Jaw. Even though I was...           \n",
       "2      I was seen relatively quickly and the staff wa...           \n",
       "3      Reception and service couldn't have been more ...           \n",
       "4      I came in they were very busy the receptionist...           \n",
       "...                                                  ...     ...   \n",
       "33498  and asked me like a hundred ridiculous questio...           \n",
       "33499  went to this ER twice and each time the doctor...           \n",
       "33500  TO HELP ME. Why would you work in the medical ...           \n",
       "33501  I usually review restaurants, but occasionally...           \n",
       "33502  60s/70s. The hospital had wisely turned off th...           \n",
       "\n",
       "                                              data_title data_category  \\\n",
       "0      urgentCare: Access Urgent Medical Care Pickeri...     Very Good   \n",
       "1      urgentCare: Access Urgent Medical Care Pickeri...          Poor   \n",
       "2      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "3      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "4      urgentCare: Access Urgent Medical Care Pickeri...      Excelent   \n",
       "...                                                  ...           ...   \n",
       "33498                hospital: Licking Memorial Hospital          Poor   \n",
       "33499                        hospital: Mount Carmel East          Poor   \n",
       "33500                        hospital: Mount Carmel East          Poor   \n",
       "33501  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "33502  hospital: University Hospitals Parma Medical C...     Very Good   \n",
       "\n",
       "       data_category_number  \n",
       "0                         4  \n",
       "1                         1  \n",
       "2                         5  \n",
       "3                         5  \n",
       "4                         5  \n",
       "...                     ...  \n",
       "33498                     1  \n",
       "33499                     1  \n",
       "33500                     1  \n",
       "33501                     4  \n",
       "33502                     4  \n",
       "\n",
       "[33503 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23095/1482219229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m d = PCAAndKMeansWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0moutput_json_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Forth_Care_review_full.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0msentence_transformer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mp_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23095/1617313285.py\u001b[0m in \u001b[0;36mPCAAndKMeansWithCustomDataset\u001b[0;34m(input_file_name, model_max_length, output_json_file_name, sentence_transformer_name, p_components, use_labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_transformer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_string'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0membeddings_for_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Reducing the dimensionality of embeddings with PCA After applying KMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 15.78 GiB total capacity; 4.93 GiB already allocated; 13.00 MiB free; 4.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "d = PCAAndKMeansWithCustomDataset(input_file_name=\"Care_review_full.csv\", \n",
    "                        model_max_length=384,\n",
    "                        output_json_file_name=\"Forth_Care_review_full.json\",\n",
    "                         sentence_transformer_name='all-MiniLM-L6-v2',\n",
    "                        p_components=2,\n",
    "                        # Keep use_labels=True always for KMeans+PCA (reason: there can be countless clusters without labels)                                   \n",
    "                        use_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "torch-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
